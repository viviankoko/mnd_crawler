import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path
import re
import pandas as pd

BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"
}

BASE_DIR = Path(__file__).parent

# è·¯å¾‘ï¼ˆéƒ½æ”¾åœ¨ repo æ ¹ç›®éŒ„ï¼‰
MANUAL_CSV = BASE_DIR / "manual_gap.csv"          # ä½ æ‰‹å‹•è£œçš„ç¼ºå£è³‡æ–™
LATEST_CSV = BASE_DIR / "pla_daily_latest.csv"    # é€™æ¬¡çˆ¬åˆ°çš„æ‰€æœ‰è³‡æ–™
FINAL_CSV = BASE_DIR / "pla_daily_clean_full.csv" # åˆä½µå¾Œæœ€çµ‚æª”æ¡ˆ


def build_list_url(page: int) -> str:
    """page=1: /plaactlist, page>=2: /plaactlist/2"""
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


def crawl_list_page(page: int):
    """
    æŠ“æŸä¸€é åˆ—è¡¨ï¼Œåªç•™ã€Œä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹ã€
    å›å‚³ list[dict]: {roc_date, url}
    roc_date ä¾‹å¦‚ '114.12.01'
    """
    url = build_list_url(page)
    print(f"\nğŸ” æŠ“åˆ—è¡¨é ï¼š{url}")

    try:
        r = requests.get(url, headers=HEADERS, timeout=20)
        r.raise_for_status()
    except Exception as e:
        # ğŸ”¥ é—œéµï¼šåˆ—è¡¨é  503 æˆ–å…¶ä»–éŒ¯èª¤æ™‚ï¼Œä¸è¦è®“æ•´å€‹ç¨‹å¼æ›æ‰
        print(f"âš ï¸ æŠ“å–åˆ—è¡¨é å¤±æ•—ï¼šç¬¬ {page} é  {url} - {e}")
        # å›å‚³ç©º listï¼Œè®“ crawl_all_pages() æŠŠé€™ä¸€é è¦–ç‚ºã€Œæ²’æœ‰è³‡æ–™ã€ä¸¦åœæ­¢å¾€å¾ŒæŠ“
        return []

    soup = BeautifulSoup(r.text, "html.parser")

    rows = []

    for a in soup.find_all("a"):
        text = a.get_text(strip=True)
        if "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹" not in text:
            continue

        # ä¾‹å¦‚ï¼š114.12.01ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹é»é–±æ¬¡æ•¸ï¼š413 æ¬¡
        m = re.search(r"\d{3}\.\d{2}\.\d{2}", text)
        if not m:
            continue
        roc_date = m.group(0)

        href = a.get("href")
        if not href:
            continue
        article_url = urljoin(BASE_URL, href)

        rows.append(
            {
                "roc_date": roc_date,
                "url": article_url,
            }
        )

    print(f"ğŸ“Œ æœ¬é æŠ“åˆ° {len(rows)} ç­†")
    return rows


def extract_maincontent_text(html: str) -> str:
    """
    åªå– <div class="maincontent"> è£¡é¢çš„æ–‡å­—ï¼Œ
    ä¸²æˆä¸€è¡Œï¼š
    ã€Œä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹ ä¸€ã€æ—¥æœŸï¼šâ€¦ äºŒã€æ´»å‹•å‹•æ…‹ï¼šâ€¦ã€
    """
    soup = BeautifulSoup(html, "html.parser")

    main_div = soup.select_one("div.maincontent")
    if main_div is None:
        return ""

    parts = list(main_div.stripped_strings)
    text = " ".join(parts)
    return text


def crawl_article_text(url: str) -> str:
    print(f"â¡ï¸ æŠ“æ–‡ç« é ï¼š{url}")
    try:
        r = requests.get(url, headers=HEADERS, timeout=20)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
    except Exception as e:
        print(f"âš ï¸ æŠ“å–æ–‡ç« å¤±æ•—ï¼š{url} - {e}")
        return ""
    return extract_maincontent_text(r.text)


def roc_to_sort_key(s: str):
    """
    æŠŠ '114/12/03' è½‰æˆæ’åºç”¨ tuple (114, 12, 3)
    """
    try:
        y, m, d = s.split("/")
        return int(y), int(m), int(d)
    except Exception:
        return (0, 0, 0)


def crawl_all_pages(max_pages: int = 200) -> pd.DataFrame:
    """
    å¾ç¬¬ 1 é ä¸€è·¯æŠ“åˆ°æ²’è³‡æ–™æˆ–é”åˆ° max_pagesã€‚
    å›å‚³æ¬„ä½ï¼šæ—¥æœŸ, å…§å®¹
    """
    data_rows = []

    for page in range(1, max_pages + 1):
        entries = crawl_list_page(page)
        if not entries:
            print(f"âšª ç¬¬ {page} é æ²’æœ‰è³‡æ–™ï¼Œåœæ­¢å¾€å¾ŒæŠ“ã€‚")
            break

        for entry in entries:
            text = crawl_article_text(entry["url"])
            date_str = entry["roc_date"].replace(".", "/")  # 114.12.03 -> 114/12/03
            data_rows.append(
                {
                    "æ—¥æœŸ": date_str,
                    "å…§å®¹": text,
                }
            )

    df = pd.DataFrame(data_rows)
    return df


def merge_with_manual(df_new: pd.DataFrame) -> pd.DataFrame:
    """
    æŠŠé€™æ¬¡çˆ¬åˆ°çš„ df_new è·Ÿ manual_gap.csv åˆä½µã€‚
    manual_gap.csv æ¯åˆ—æ ¼å¼ï¼š
    114/11/29,ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹ ä¸€ã€æ—¥æœŸï¼šâ€¦
    ï¼ˆæ²’æœ‰æ¬„ä½åç¨±ï¼‰
    """
    if MANUAL_CSV.exists():
        print(f"ğŸ“¥ è®€å–æ‰‹å‹•è£œé½Šæª”æ¡ˆï¼š{MANUAL_CSV}")
        df_manual = pd.read_csv(MANUAL_CSV, header=None, names=["æ—¥æœŸ", "å…§å®¹"])
    else:
        print("âš ï¸ æ‰¾ä¸åˆ° manual_gap.csvï¼Œåªä½¿ç”¨æœ¬æ¬¡çˆ¬åˆ°çš„è³‡æ–™ã€‚")
        df_manual = pd.DataFrame(columns=["æ—¥æœŸ", "å…§å®¹"])

    # å€‹åˆ¥å»é‡
    df_manual = df_manual.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")
    df_new = df_new.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")

    # åˆä½µï¼šæ‰‹å‹•è£œé½Šåœ¨å‰ï¼Œæ–°çˆ¬è³‡æ–™åœ¨å¾Œ
    df_all = pd.concat([df_manual, df_new], ignore_index=True)

    # ä»¥ã€Œæ—¥æœŸã€å»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç¾ï¼ˆå„ªå…ˆ manualï¼‰
    df_all = df_all.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")

    # ä¾æ—¥æœŸæ’åºï¼ˆæ°‘åœ‹å¹´ / æœˆ / æ—¥ï¼‰
    df_all = df_all.sort_values(by="æ—¥æœŸ", key=lambda col: col.map(roc_to_sort_key))

    return df_all


def main():
    print("ğŸš€ é–‹å§‹çˆ¬å–åœ‹é˜²éƒ¨å€åŸŸå‹•æ…‹â€¦")

    df_new = crawl_all_pages()
    print(f"\nâœ… æœ¬æ¬¡å…±çˆ¬åˆ° {len(df_new)} ç­†è³‡æ–™")

    if len(df_new) > 0:
        # æœ¬æ¬¡çˆ¬åˆ°çš„åŸå§‹è³‡æ–™
        df_new.to_csv(LATEST_CSV, index=False, header=False, encoding="utf-8-sig")
        print(f"ğŸ“ å·²å¯«å…¥æœ€æ–°çˆ¬å–è³‡æ–™ï¼š{LATEST_CSV}")
    else:
        print("âš ï¸ æœ¬æ¬¡æ²’æœ‰çˆ¬åˆ°ä»»ä½•è³‡æ–™ã€‚ä»æœƒå˜—è©¦ç”¨ manual_gap.csv ç”¢å‡ºæœ€çµ‚æª”ã€‚")

    # åˆä½µæ‰‹å‹•è£œé½Š
    df_final = merge_with_manual(df_new)

    # æœ€çµ‚è¼¸å‡ºï¼šä¸å¯«æ¬„ä½åç¨±
    df_final.to_csv(FINAL_CSV, index=False, header=False, encoding="utf-8-sig")
    print(f"ğŸ å·²å¯«å…¥åˆä½µå¾Œæœ€çµ‚æª”æ¡ˆï¼š{FINAL_CSV}")
    print(f"ğŸ“Š æœ€çµ‚è³‡æ–™ç­†æ•¸ï¼š{len(df_final)}")


if __name__ == "__main__":
    main()
