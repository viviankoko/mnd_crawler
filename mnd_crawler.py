# -*- coding: utf-8 -*-
"""mnd_crawler

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lhNZt1SQSxr7TwyDLd_xLSl4YUx2GGrs
"""

def extract_metrics(text):
    return {
        "偵測到的共機總數": None,
        "進入ADIZ或跨越中線": None,
        "偵測到的共艦數量": None
    }

# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup #用來爬蟲，
import re
import pandas as pd
import time

BASE_URL = "https://www.mnd.gov.tw/PublishTable.aspx?Types=即時軍事動態&title=國防消息"
HEADERS = {"User-Agent": "Mozilla/5.0"}

def parse_viewstate_fields(soup):
    def val(name):
        el = soup.find("input", {"name": name})
        return el["value"] if el and el.has_attr("value") else ""
    return {
        "__VIEWSTATE": val("__VIEWSTATE"),
        "__VIEWSTATEGENERATOR": val("__VIEWSTATEGENERATOR"),
        "__EVENTVALIDATION": val("__EVENTVALIDATION"),
    }

def extract_postback_target(a_tag):
    href = a_tag.get("href", "")
    m = re.search(r"__doPostBack\('([^']+)'", href)
    return m.group(1) if m else None

def parse_list_page(html):
    soup = BeautifulSoup(html, "html.parser")
    fields = parse_viewstate_fields(soup)
    items = []
    for tr in soup.select("table tr"):
        a = tr.find("a", href=True)
        if not a:
            continue
        title = a.get_text(strip=True)
        if "中共解放軍臺海周邊海、空域動態" not in title:
            continue
        target = extract_postback_target(a)
        date_text = None
        for td in tr.find_all("td"):
            if re.search(r"\d{3}/\d{1,2}/\d{1,2}", td.get_text()):
                date_text = td.get_text(strip=True)
                break
        items.append({"date": date_text, "target": target, "view": fields})
    return items

def fetch_detail(session, view_fields, target):
    data = {
        "__EVENTTARGET": target,
        "__EVENTARGUMENT": "",
        "__VIEWSTATE": view_fields["__VIEWSTATE"],
        "__VIEWSTATEGENERATOR": view_fields["__VIEWSTATEGENERATOR"],
        "__EVENTVALIDATION": view_fields["__EVENTVALIDATION"]
    }
    r = session.post(BASE_URL, headers=HEADERS, data=data, timeout=20)
    r.raise_for_status()
    return r.text

def extract_clean_paragraph(html):
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text(" ", strip=True)

    start = text.find("中共解放軍臺海周邊海、空域動態")
    end = text.find("國軍運用任務機、艦及岸置飛彈系統嚴密監控與應處。")

    if start != -1 and end != -1:
        return text[start:end + len("國軍運用任務機、艦及岸置飛彈系統嚴密監控與應處。")]
    elif start != -1:
        return text[start:]
    else:
        return text

def crawl_all(max_pages=None):
    session = requests.Session()
    page = 1
    records = []

    while True:

        # 測試模式：只跑指定頁數
        if max_pages is not None and page > max_pages:
            break

        print(f"\n抓取第 {page} 頁: {BASE_URL}&Page={page}")
        html = session.get(f"{BASE_URL}&Page={page}", timeout=10).text

        items = parse_list_page(html)
        if not items:
            print("沒有更多資料，結束。")
            break

        for idx, it in enumerate(items, start=1):
            print(f"({idx}/{len(items)}) 抓取 {it['date']}")
            detail_html = fetch_detail(session, it["eventtarget"], it["eventargument"])
            clean_text = extract_clean_paragraph(detail_html)

            metrics = extract_metrics(clean_text)

            record = {
                "日期": it["date"],
                "通報內容": clean_text,
                **metrics
            }

            records.append(record)

        page += 1

    df = pd.DataFrame(records)
    print(f"\n全部完成！共抓取 {len(df)} 筆資料。")
    print(df.head())
    return df

if __name__ == "__main__":

    df = crawl_all(max_pages=1)

    print("\n=== 第一筆的通報內容（用來檢查乾淨文本） ===")
    print(df.loc[0, "通報內容"])  # 這行就是你要看的乾淨文本
    #
    #df = crawl_all()
    ##print("\n全部完成！共抓取", len(df), "筆資料。")
    #print(df.head(5))
