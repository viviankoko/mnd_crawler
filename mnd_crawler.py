# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path
import re
import pandas as pd
import time

BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"
}

BASE_DIR = Path(__file__).parent

# æª”æ¡ˆè·¯å¾‘ï¼ˆéƒ½æ”¾åœ¨ repo æ ¹ç›®éŒ„ï¼‰
MANUAL_CSV = BASE_DIR / "manual_gap.csv"          # æ‰‹å‹•è£œçš„ç¼ºå£
LATEST_CSV = BASE_DIR / "pla_daily_latest.csv"    # é€™æ¬¡çˆ¬åˆ°çš„æœ€æ–° raw è³‡æ–™ï¼ˆç„¡æ¬„ä½åç¨±ï¼‰
FINAL_CSV = BASE_DIR / "pla_daily_clean_full.csv" # åˆä½µå¾Œæœ€çµ‚æª”æ¡ˆï¼ˆæœ‰ã€Œæ—¥æœŸ,å…§å®¹ã€æ¬„ä½ï¼‰

# é—œéµå­—ï¼šæ²¿ç”¨ä½  ASPX èˆŠç‰ˆçˆ¬èŸ²çš„ç¯©é¸æ¨™æº–
KEYWORDS = [
    "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
    "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
    "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "è¸°è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "é€¾è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
]

# å…±ç”¨ Sessionï¼ˆæ•ˆèƒ½å¥½ä¸€é»ï¼‰
SESSION = requests.Session()


# ------------------------------------------------------------
# å·¥å…·ï¼šå¸¶ retry çš„ GETï¼ˆåˆ—è¡¨é ã€å…§é å…±ç”¨ï¼‰
# ------------------------------------------------------------
def safe_get(url: str, max_retries: int = 5, timeout: int = 40, sleep_base: float = 2.0):
    """
    å¸¶é‡è©¦æ©Ÿåˆ¶çš„ GETï¼š
    - å¤±æ•—æ™‚æœƒæœ€å¤šé‡è©¦ max_retries æ¬¡
    - æœ€å¾Œä»å¤±æ•—å°±å›å‚³ Noneï¼ˆå‘¼å«ç«¯è‡ªå·±æ±ºå®šæ€éº¼è™•ç†ï¼‰
    """
    for attempt in range(1, max_retries + 1):
        try:
            r = SESSION.get(url, headers=HEADERS, timeout=timeout)
            r.raise_for_status()
            r.encoding = r.apparent_encoding
            return r.text
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {attempt} æ¬¡æŠ“å–å¤±æ•—ï¼š{url} - {e}")
            if attempt == max_retries:
                print(f"âŒ æ”¾æ£„æŠ“å–ï¼š{url}")
                return None
            # éå¢ç­‰å¾…æ™‚é–“ï¼ˆ2 ç§’ã€4 ç§’ã€6 ç§’â€¦ï¼‰
            time.sleep(sleep_base * attempt)


# ------------------------------------------------------------
# åˆ—è¡¨é 
# ------------------------------------------------------------
def build_list_url(page: int) -> str:
    """page=1: /plaactlist, page>=2: /plaactlist/2"""
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


def crawl_list_page(page: int):
    """
    æŠ“æŸä¸€é åˆ—è¡¨ï¼Œåªç•™æˆ‘å€‘é—œå¿ƒçš„é—œéµå­—æ¨™é¡Œã€‚
    å›å‚³ï¼š
        - list[dict]ï¼Œæ¯å€‹å…ƒç´ ï¼š{roc_date, url}
        - è‹¥æ•´é è®€å–å¤±æ•—ï¼šå›å‚³ Noneï¼ˆçµ¦ä¸Šå±¤åˆ¤æ–·ã€Œç•¥éé€™ä¸€é ã€ï¼‰
    """
    url = build_list_url(page)
    print(f"\nğŸ” æŠ“åˆ—è¡¨é ï¼š{url}")

    html = safe_get(url, max_retries=5, timeout=40)
    if html is None:
        # æ˜ç¢ºæ¨™è¨˜é€™ä¸€é å¤±æ•—ï¼ˆèˆ‡ã€Œæ­£å¸¸ä½†å‰›å¥½æ²’æœ‰è³‡æ–™ã€å€åˆ†ï¼‰
        print(f"âšª ç¬¬ {page} é æŠ“å–å¤±æ•—ï¼Œç•¥éã€‚")
        return None

    soup = BeautifulSoup(html, "html.parser")
    rows = []

    # ç›®å‰ç¶²ç«™åˆ—è¡¨çš„æ—¥æœŸé€šå¸¸ç›´æ¥å¯«åœ¨ a æ–‡å­—å‰é¢ï¼Œä¾‹å¦‚ï¼š
    # 111.11.08 11æœˆ8æ—¥è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹æ–°èç¨¿
    for a in soup.find_all("a", href=True):
        title = a.get_text(strip=True)
        if not any(kw in title for kw in KEYWORDS):
            continue

        # æŠ“ ROC æ—¥æœŸ 111.11.08
        m = re.search(r"\d{3}\.\d{2}\.\d{2}", title)
        if not m:
            # æœ‰äº›èˆŠæ–‡å¯èƒ½æ²’å¸¶é€™ç¨®æ ¼å¼ï¼Œç›´æ¥ç•¥é
            continue
        roc_date = m.group(0)

        href = a.get("href")
        article_url = urljoin(BASE_URL, href)

        rows.append(
            {
                "roc_date": roc_date,
                "url": article_url,
            }
        )

    print(f"ğŸ“Œ æœ¬é æŠ“åˆ° {len(rows)} ç­†")
    return rows


# ------------------------------------------------------------
# æ–‡ç« é æ“·å–
# ------------------------------------------------------------
def extract_maincontent_text(html: str) -> str:
    """
    åªå– <div class="maincontent"> è£¡é¢çš„æ–‡å­—ï¼Œ
    ä¸²æˆä¸€è¡Œï¼š
    ã€Œåœ‹é˜²éƒ¨ä»Šï¼ˆ8ï¼‰æ—¥è¡¨ç¤ºï¼Œè¿„1700æ™‚æ­¢ï¼Œåµç²å…±æ©Ÿâ€¦ã€
    """
    soup = BeautifulSoup(html, "html.parser")
    main_div = soup.select_one("div.maincontent")
    if main_div is None:
        return ""

    parts = list(main_div.stripped_strings)
    return " ".join(parts)


def crawl_article_text(url: str) -> str:
    print(f"â¡ï¸ æŠ“æ–‡ç« é ï¼š{url}")
    html = safe_get(url, max_retries=3, timeout=30)
    if html is None:
        # å…§é çœŸçš„æŠ“ä¸åˆ°å°±ç•™ç©ºå­—ä¸²ï¼Œä½†ä¸è®“æ•´å€‹æµç¨‹æ›æ‰
        return ""
    return extract_maincontent_text(html)


# ------------------------------------------------------------
# æ—¥æœŸæ’åºï¼ˆæ°‘åœ‹å¹´ï¼‰
# ------------------------------------------------------------
def roc_to_sort_key(s: str):
    """
    æŠŠ '114/12/03' è½‰æˆæ’åºç”¨ tuple (114, 12, 3)
    """
    try:
        y, m, d = s.split("/")
        return int(y), int(m), int(d)
    except Exception:
        return (0, 0, 0)


# ------------------------------------------------------------
# ä¸»æµç¨‹ï¼šæŠ“æ‰€æœ‰é é¢
# ------------------------------------------------------------
def crawl_all_pages(max_pages: int = 200):
    """
    å¾ç¬¬ 1 é ä¸€è·¯æŠ“åˆ° max_pagesã€‚
    ç‰¹é»ï¼š
      - æŸä¸€é æ•´é  timeout â†’ è¨˜éŒ„åœ¨ skipped_pagesï¼Œç¹¼çºŒä¸‹ä¸€é 
      - è‹¥é€£çºŒ 3 é æ˜¯ã€Œæ­£å¸¸ä½†æ²’æœ‰ä»»ä½•ç¬¦åˆé—œéµå­—çš„è³‡æ–™ã€æ‰åœæ­¢
    å›å‚³ï¼š
      df_new: DataFrame(æ¬„ä½ï¼šæ—¥æœŸ, å…§å®¹ï¼Œæ—¥æœŸç‚ºæ°‘åœ‹å¹´æ ¼å¼ 114/12/03)
      skipped_pages: list[int] è¢«ç•¥éçš„é ç¢¼
    """
    data_rows = []
    skipped_pages = []
    empty_streak = 0

    for page in range(1, max_pages + 1):
        entries = crawl_list_page(page)

        # æ•´é æŠ“å–å¤±æ•—ï¼šç•¥é
        if entries is None:
            skipped_pages.append(page)
            continue

        # æ­£å¸¸ä½†æ²’æœ‰ç¬¦åˆé—œéµå­—çš„è³‡æ–™
        if not entries:
            empty_streak += 1
            print(f"ğŸ”š ç¬¬ {page} é ç„¡ç¬¦åˆé—œéµå­—çš„è³‡æ–™ï¼ˆé€£çºŒ {empty_streak} é ï¼‰")
            # é€™è£¡æ¡ã€Œé€£çºŒ 3 é ç©ºã€å°±åœæ­¢ï¼Œé¿å…æŸä¸€é å‰›å¥½æ²’æœ‰è³‡æ–™å°±ææ—©çµæŸ
            if empty_streak >= 3:
                print("ğŸ“´ é€£çºŒ 3 é ç„¡è³‡æ–™ï¼Œåœæ­¢å¾ŒçºŒæŠ“å–ã€‚")
                break
            else:
                continue

        # æœ‰è³‡æ–™ â†’ reset ç©ºé è¨ˆæ•¸
        empty_streak = 0

        for entry in entries:
            text = crawl_article_text(entry["url"])
            date_str = entry["roc_date"].replace(".", "/")  # 111.11.08 -> 111/11/08

            data_rows.append(
                {
                    "æ—¥æœŸ": date_str,
                    "å…§å®¹": text,
                }
            )

        # é¿å…å¤ªå…‡çŒ›è¢«ç•¶æ©Ÿå™¨äººï¼Œé èˆ‡é é–“ç¨å¾®ç¡ä¸€ä¸‹
        time.sleep(1.0)

    df = pd.DataFrame(data_rows)
    return df, skipped_pages


# ------------------------------------------------------------
# åˆä½µ manual_gap.csv
# ------------------------------------------------------------
def merge_with_manual(df_new: pd.DataFrame) -> pd.DataFrame:
    """
    æŠŠé€™æ¬¡çˆ¬åˆ°çš„ df_new è·Ÿ manual_gap.csv åˆä½µã€‚
    manual_gap.csv æ¯åˆ—æ ¼å¼ï¼š
        114/11/29,ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹ ä¸€ã€æ—¥æœŸï¼šâ€¦
    ï¼ˆæ²’æœ‰æ¬„ä½åç¨±ï¼‰
    """
    if MANUAL_CSV.exists():
        print(f"ğŸ“¥ è®€å–æ‰‹å‹•è£œé½Šæª”æ¡ˆï¼š{MANUAL_CSV}")
        df_manual = pd.read_csv(MANUAL_CSV, header=None, names=["æ—¥æœŸ", "å…§å®¹"])
    else:
        print("âš ï¸ æœªæ‰¾åˆ° manual_gap.csvï¼Œåƒ…ä½¿ç”¨æœ¬æ¬¡çˆ¬åˆ°çš„è³‡æ–™ã€‚")
        df_manual = pd.DataFrame(columns=["æ—¥æœŸ", "å…§å®¹"])

    # å€‹åˆ¥å»é‡
    df_manual = df_manual.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")
    df_new = df_new.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")

    # manual åœ¨å‰ï¼Œæ–°çˆ¬åœ¨å¾Œ
    df_all = pd.concat([df_manual, df_new], ignore_index=True)

    # å†ä»¥ã€Œæ—¥æœŸã€å»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡ï¼ˆå„ªå…ˆ manualï¼‰
    df_all = df_all.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")

    # ä¾æ°‘åœ‹å¹´æœˆæ—¥æ’åº
    df_all = df_all.sort_values(by="æ—¥æœŸ", key=lambda col: col.map(roc_to_sort_key))

    return df_all


# ------------------------------------------------------------
# main()
# ------------------------------------------------------------
def main():
    print("ğŸš€ é–‹å§‹çˆ¬å–åœ‹é˜²éƒ¨å€åŸŸå‹•æ…‹â€¦")

    df_new, skipped_pages = crawl_all_pages(max_pages=200)
    print(f"\nâœ… æœ¬æ¬¡å…±çˆ¬åˆ° {len(df_new)} ç­†è³‡æ–™")

    if skipped_pages:
        print(f"âš ï¸ æœ‰è¢«ç•¥éçš„åˆ—è¡¨é ï¼ˆå®Œæ•´ timeoutï¼‰ï¼š{skipped_pages}")

    # æœ€æ–°ä¸€è¼ª raw è³‡æ–™ï¼ˆç¶­æŒç„¡æ¬„ä½åç¨±ï¼‰
    if len(df_new) > 0:
        df_new.to_csv(LATEST_CSV, index=False, header=False, encoding="utf-8-sig")
        print(f"ğŸ“ å·²å¯«å…¥æœ€æ–°çˆ¬å–è³‡æ–™ï¼ˆç„¡æ¬„ä½åç¨±ï¼‰ï¼š{LATEST_CSV}")
    else:
        print("âš ï¸ æœ¬æ¬¡æ²’æœ‰çˆ¬åˆ°ä»»ä½•æ–°è³‡æ–™ï¼ŒLATEST æª”ä¸æœƒè¦†è“‹ã€‚")

    # åˆä½µ manual_gap + æœ¬æ¬¡æ–°è³‡æ–™ â†’ æœ€çµ‚å®Œæ•´è³‡æ–™ï¼ˆæœ‰æ¬„ä½åç¨±ï¼‰
    df_final = merge_with_manual(df_new)
    df_final.to_csv(FINAL_CSV, index=False, header=True, encoding="utf-8-sig")

    print(f"ğŸ å·²å¯«å…¥æœ€çµ‚å®Œæ•´è³‡æ–™ï¼ˆå«æ¨™é¡Œåˆ—ï¼‰ï¼š{FINAL_CSV}")
    print(f"ğŸ“Š æœ€çµ‚è³‡æ–™ç­†æ•¸ï¼š{len(df_final)}")


if __name__ == "__main__":
    main()
