#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
mnd_crawler.py

åŠŸèƒ½ï¼š
- python mnd_crawler.py full
    â†’ å…¨é‡é‡å»ºï¼ˆå¾žç¬¬ 1 é çˆ¬åˆ°ç„¡è³‡æ–™ç‚ºæ­¢ï¼‰ï¼‹åˆä½µ manual_gap.csv
- python mnd_crawler.py
    â†’ æ¯æ—¥æ›´æ–°ï¼ˆæŠ“ç¬¬ 1 é æ–°æ—¥æœŸï¼‰ï¼‹åˆä½µ manual_gap.csv
"""

import sys
import time
import re
from pathlib import Path
from typing import List, Dict, Tuple

import requests
from bs4 import BeautifulSoup
import pandas as pd


# ---------------- åŸºæœ¬è¨­å®š ----------------
BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"
}

BASE_DIR = Path(__file__).parent
OUTPUT_CSV = BASE_DIR / "mnd_pla.csv"
MANUAL_GAP = BASE_DIR / "manual_gap.csv"

KEYWORDS = [
    "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
    "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
    "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "è¸°è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "é€¾è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
]


# ---------------- å·¥å…· ----------------

def safe_get(url: str, timeout: int = 20) -> str | None:
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout)
        r.raise_for_status()
        r.encoding = "utf-8"
        return r.text
    except Exception as e:
        print(f"âš ï¸ æŠ“å–å¤±æ•—ï¼š{url} - {e}")
        return None


def build_list_url(page: int) -> str:
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


# ---------------- åˆ—è¡¨é  ----------------

def crawl_list_page(page: int) -> List[Dict]:
    url = build_list_url(page)
    print(f"\nðŸ” æŠ“åˆ—è¡¨é ï¼š{url}")

    html = safe_get(url)
    if html is None:
        return []

    soup = BeautifulSoup(html, "html.parser")
    rows = []

    for a in soup.find_all("a", href=True):
        text = a.get_text(strip=True)
        if not any(kw in text for kw in KEYWORDS):
            continue

        m = re.search(r"\d{3}\.\d{2}\.\d{2}", text)
        if not m:
            parent = a.parent.get_text(strip=True)
            m = re.search(r"\d{3}\.\d{2}\.\d{2}", parent)
            if not m:
                continue

        roc_date = m.group(0)
        url2 = requests.compat.urljoin(BASE_URL, a["href"])
        rows.append({"roc_date": roc_date, "url": url2})

    print(f"ðŸ“Œ ç¬¬ {page} é æŠ“åˆ° {len(rows)} ç­†")
    return rows


# ---------------- å…§æ–‡ ----------------

def extract_maincontent_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    main = soup.select_one("div.maincontent")
    if not main:
        return ""
    return " ".join(main.stripped_strings)


def crawl_article_text(url: str) -> str:
    print(f"âž¡ï¸ æŠ“æ–‡ç« é ï¼š{url}")
    html = safe_get(url)
    return extract_maincontent_text(html) if html else ""


# ---------------- æ—¥æœŸæŽ’åº ----------------

def date_sort_key(s: str) -> Tuple[int, int, int]:
    try:
        y, m, d = s.split("/")
        y = int(y)
        if len(y.__str__()) == 3:
            y += 1911
        return (y, int(m), int(d))
    except:
        return (0, 0, 0)


# ---------------- è£œä¸ä½µå…¥ï¼ˆè£œä¸å„ªå…ˆï¼‰ ----------------

def merge_with_manual(df_core: pd.DataFrame) -> pd.DataFrame:
    df = df_core.copy()
    df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str).str.strip()

    if MANUAL_GAP.exists():
        gap = pd.read_csv(MANUAL_GAP, encoding="utf-8-sig")
        gap["æ—¥æœŸ"] = gap["æ—¥æœŸ"].astype(str).str.strip()
        df = pd.concat([df, gap[["æ—¥æœŸ", "å…§å®¹"]]], ignore_index=True)
        print(f"ðŸ“¥ è£œä¸ç­†æ•¸ï¼š{len(gap)}")
    else:
        print("â„¹ï¸ manual_gap.csv ä¸å­˜åœ¨ï¼ˆç•¥éŽè£œä¸ï¼‰")

    df = df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
    df = df.sort_values("æ—¥æœŸ", key=lambda col: col.map(date_sort_key))
    df = df.reset_index(drop=True)
    return df


# ---------------- FULLï¼šç„¡ä¸Šé™å¾€ä¸‹çˆ¬ ----------------

def run_full():
    print("ðŸš€ [FULL] å…¨é‡é–‹å§‹ï¼ˆç›´åˆ°ç„¡è³‡æ–™é ï¼‰")

    rows = []
    page = 1

    while True:
        entries = crawl_list_page(page)
        if not entries:
            print(f"âšª ç¬¬ {page} é å·²ç„¡è³‡æ–™ï¼Œåœæ­¢ full")
            break

        for e in entries:
            date_str = e["roc_date"].replace(".", "/")
            content = crawl_article_text(e["url"])
            rows.append({"æ—¥æœŸ": date_str, "å…§å®¹": content})
            time.sleep(0.15)

        page += 1
        time.sleep(0.2)

    df_core = pd.DataFrame(rows)
    print(f"ðŸ“Œ FULL çˆ¬åˆ° {len(df_core)} ç­†")

    df_final = merge_with_manual(df_core)
    df_final.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

    print(f"ðŸ FULL å®Œæˆ â†’ {OUTPUT_CSV}ï¼ˆ{len(df_final)} ç­†ï¼‰")


# ---------------- DAILYï¼šæŠ“ç¬¬ 1 é æ–°è³‡æ–™ ----------------

def run_daily():
    print("ðŸ“… [DAILY] æ¯æ—¥æ›´æ–°")

    if not OUTPUT_CSV.exists():
        print("âš ï¸ ä¸»æª”ä¸å­˜åœ¨ â†’ æ”¹è·‘ FULL")
        run_full()
        return

    df_old = pd.read_csv(OUTPUT_CSV, encoding="utf-8-sig")
    df_old["æ—¥æœŸ"] = df_old["æ—¥æœŸ"].astype(str).str.strip()
    old_dates = set(df_old["æ—¥æœŸ"])

    entries = crawl_list_page(1)
    if not entries:
        print("âš ï¸ ç¬¬ 1 é ç„¡è³‡æ–™")
        return

    new_rows = []
    for e in entries:
        date_str = e["roc_date"].replace(".", "/")
        if date_str in old_dates:
            continue

        content = crawl_article_text(e["url"])
        new_rows.append({"æ—¥æœŸ": date_str, "å…§å®¹": content})
        time.sleep(0.15)

    if not new_rows:
        print("âœ… ç„¡æ–°æ—¥æœŸ")
        return

    df_core = pd.concat([df_old, pd.DataFrame(new_rows)], ignore_index=True)
    df_final = merge_with_manual(df_core)
    df_final.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

    print(f"ðŸ DAILY å®Œæˆï¼ˆ{len(df_final)} ç­†ï¼‰")


# ---------------- main ----------------

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1].lower() == "full":
        run_full()
    else:
        run_daily()
