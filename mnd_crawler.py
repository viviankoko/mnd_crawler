#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
mnd_crawler.py

ç”¨é€”ï¼š
1. ç¬¬ä¸€æ¬¡è·‘ï¼ˆå…¨é‡ï¼‰ï¼šå¾åœ‹é˜²éƒ¨ã€Œå€åŸŸå‹•æ…‹ã€æŠŠç›®å‰æ‰€æœ‰è³‡æ–™çˆ¬å®Œï¼Œ
   ç„¶å¾ŒæŠŠ manual_gap.csv ä½µé€²å» â†’ è¼¸å‡º mnd_pla.csvã€‚
2. ä¹‹å¾Œæ¯å¤©è·‘ï¼ˆå¢é‡ï¼‰ï¼šåªæŠ“æœ€è¿‘å¹¾é ï¼Œæ‰¾å‡ºã€Œé‚„æ²’å¯«é€² mnd_pla.csvã€çš„æ–°è³‡æ–™ï¼Œ
   append é€²å»ï¼Œå†ä½µä¸€æ¬¡ manual_gap.csv â†’ è¦†è“‹å› mnd_pla.csvã€‚

ä½¿ç”¨æ–¹å¼ï¼š
- ç¬¬ä¸€æ¬¡å…¨é‡é‡å»ºï¼š
    åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œï¼š
      MND_MODE=full python mnd_crawler.py
    ï¼ˆWindows å¯ä»¥ç”¨ï¼šset MND_MODE=full && python mnd_crawler.pyï¼‰

- ä¹‹å¾Œæ¯æ—¥æ’ç¨‹ï¼š
    ç›´æ¥ï¼š
      python mnd_crawler.py
    ï¼ˆæˆ–åœ¨ GitHub Actions è£¡ä¸è¨­å®š MND_MODEï¼‰
"""

import os
import time
import re
from typing import List, Dict

import requests
from bs4 import BeautifulSoup
import pandas as pd

BASE = "https://www.mnd.gov.tw"
LIST_URL = BASE + "/news/plaactlist"

DATA_PATH = "mnd_pla.csv"     # ä¸»è³‡æ–™è¡¨
GAP_PATH  = "manual_gap.csv"  # ä½ çš„è£œä¸æª”

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0 Safari/537.36"
    )
}


# ---------- æŠ“å–ï¼†è§£æ ----------

def fetch(url: str, session: requests.Session | None = None) -> str:
    """æŠ“ç¶²é ï¼Œå¼·åˆ¶ç”¨ UTF-8 è§£ç¢¼ã€‚"""
    s = session or requests.Session()
    r = s.get(url, headers=HEADERS, timeout=20)
    r.encoding = "utf-8"
    return r.text


def parse_list_page(html: str) -> List[Dict]:
    """
    è§£æåˆ—è¡¨é ï¼Œå›å‚³æ¯ä¸€ç­†çš„ date / title / urlã€‚

    é‡é»ï¼š
    - selector ç”¨ a[href*="plaact/"]ï¼Œå› ç‚ºåˆ—è¡¨è£¡ href å¤šåŠæ˜¯ç›¸å°è·¯å¾‘
      ä¾‹å¦‚ "news/plaact/85454" æˆ– "/news/plaact/85454"ã€‚
    """
    soup = BeautifulSoup(html, "lxml")
    records: List[Dict] = []

    for a in soup.select("a[href*='plaact/']"):
        href = a.get("href") or ""
        if not href:
            continue

        # çµ„æˆå®Œæ•´ç¶²å€ï¼ˆæ”¯æ´ç›¸å°ï¼‹çµ•å°ï¼‰
        if href.startswith("http"):
            url = href
        else:
            url = BASE + href.lstrip("/")

        # å¾€ä¸Šæ‰¾çˆ¶å±¤ï¼Œå¾æ–‡å­—è£¡æŠ“æ—¥æœŸï¼ˆ109.09.17 / 109/09/17ï¼‰
        row = a.find_parent("tr") or a.find_parent("div")
        date_str = ""
        if row:
            m = re.search(r"\d{3}[./]\d{2}[./]\d{2}", row.get_text())
            if m:
                date_str = m.group(0).replace(".", "/")

        title = a.get_text(strip=True)
        records.append(
            {
                "date": date_str,  # ç™¼å¸ƒæ—¥ï¼ˆåˆ—è¡¨ä¸Šçš„ï¼‰
                "title": title,
                "url": url,
            }
        )

    return records


def parse_article(html: str) -> Dict[str, str]:
    """
    è§£æå…§é ï¼ŒæŠ“ maincontent çš„å®Œæ•´æ–‡å­—ï¼Œä»¥åŠ pageinfo è£¡çš„æ—¥æœŸã€‚

    å›å‚³ï¼š
      {"date": "109/09/17", "content": "...å…¨æ–‡..."}
    """
    soup = BeautifulSoup(html, "lxml")

    main = soup.select_one(".maincontent")
    if main:
        content_text = main.get_text("\n", strip=True)
    else:
        content_text = ""

    date_str = ""
    pageinfo = soup.select_one(".pageinfo")
    if pageinfo:
        spans = pageinfo.select("span")
        if len(spans) >= 2:
            raw = spans[1].get_text(strip=True)  # ä¾‹å¦‚ "109.09.17"
            m = re.search(r"\d{3}[./]\d{2}[./]\d{2}", raw)
            if m:
                date_str = m.group(0).replace(".", "/")

    return {"date": date_str, "content": content_text}


def crawl_pages(max_page: int) -> pd.DataFrame:
    """
    å¾ç¬¬ 1 é çˆ¬åˆ° max_pageã€‚
    - ç¬¬ 1 é ï¼š https://www.mnd.gov.tw/news/plaactlist
    - å¾ŒçºŒé ï¼š https://www.mnd.gov.tw/news/plaactlist/2, /3, ...
    - æ¯é è§£æåˆ—è¡¨ï¼ŒæŠ“å‡ºæ¯å‰‡çš„ urlï¼Œå†å»çˆ¬å…§é ã€‚
    - ä»¥ url å»é‡ï¼Œé¿å…é‡è¤‡ã€‚
    - è‹¥æŸé å®Œå…¨æŠ“ä¸åˆ°ä»»ä½• plaact é€£çµï¼Œå°±ç•¶ä½œåˆ°å°¾ç«¯ç›´æ¥ breakã€‚
    """
    all_rows: List[Dict] = []
    session = requests.Session()

    for page in range(1, max_page + 1):
        if page == 1:
            list_url = LIST_URL
        else:
            list_url = f"{LIST_URL}/{page}"

        print(f"ğŸ” æŠ“åˆ—è¡¨é ï¼š{list_url}")
        try:
            html = fetch(list_url, session=session)
        except Exception as e:
            print(f"âš ï¸ åˆ—è¡¨é æŠ“å–å¤±æ•— {list_url}: {e}")
            continue

        base_records = parse_list_page(html)
        if not base_records:
            print(f"é  {page} æ²’æŠ“åˆ°ä»»ä½• plaact é€£çµï¼Œè¦–ç‚ºåˆ°å°¾ç«¯ï¼Œåœæ­¢ã€‚")
            break

        print(f"é  {page} æŠ“åˆ° {len(base_records)} ç­†")

        for rec in base_records:
            art_url = rec["url"]
            try:
                art_html = fetch(art_url, session=session)
            except Exception as e:
                print(f"  âš ï¸ å…§é æŠ“å–å¤±æ•— {art_url}: {e}")
                continue

            art = parse_article(art_html)
            row = {
                "date": art["date"] or rec["date"],  # å…§é æ—¥æœŸå„ªå…ˆï¼Œæ²’æœ‰å°±ç”¨åˆ—è¡¨çš„
                "title": rec["title"],
                "url": rec["url"],
                "content": art["content"],
            }
            all_rows.append(row)

            # ç¦®è²Œæ€§ sleepï¼Œåˆ¥æŠŠå®˜æ–¹ç¶²ç«™æ‰“çˆ†
            time.sleep(0.3)

    df = pd.DataFrame(all_rows)

    if not df.empty and "url" in df.columns:
        df = df.drop_duplicates(subset=["url"], keep="last").reset_index(drop=True)

    return df


# ---------- manual_gap åˆä½µ ----------

def load_manual_gap() -> pd.DataFrame:
    """è®€é€² manual_gap.csvï¼Œå¦‚æœæ²’æœ‰å°±å›å‚³ç©º DataFrameã€‚"""
    if not os.path.exists(GAP_PATH):
        print("ğŸ” manual_gap.csv ä¸å­˜åœ¨ï¼Œç•¥éè£œä¸ã€‚")
        return pd.DataFrame()

    print(f"ğŸ“¥ è®€å–è£œä¸æª”ï¼š{GAP_PATH}")
    gap_df = pd.read_csv(GAP_PATH, encoding="utf-8-sig")
    print(f"  â†’ {len(gap_df)} ç­†è£œä¸è³‡æ–™")
    return gap_df


def merge_with_gap(main_df: pd.DataFrame, gap_df: pd.DataFrame) -> pd.DataFrame:
    """
    æŠŠä¸»è³‡æ–™è¡¨èˆ‡ manual_gap åˆä½µã€‚

    è¦å‰‡ï¼š
    - ä»¥ url ç•¶å”¯ä¸€ keyã€‚
    - manual_gap æ”¾åœ¨å¾Œé¢ï¼šå¦‚æœåŒä¸€å€‹ url ä¸»æª”å’Œè£œä¸éƒ½æœ‰ï¼Œä»¥è£œä¸ç‰ˆæœ¬ç‚ºæº–ã€‚
    """
    if gap_df.empty:
        return main_df.reset_index(drop=True)

    merged = pd.concat([main_df, gap_df], ignore_index=True)

    if "url" in merged.columns:
        merged = merged.drop_duplicates(subset=["url"], keep="last")
    elif set(["date", "title"]).issubset(merged.columns):
        merged = merged.drop_duplicates(subset=["date", "title"], keep="last")
    else:
        merged = merged.drop_duplicates(keep="last")

    if "date" in merged.columns:
        merged = merged.sort_values("date").reset_index(drop=True)

    return merged


# ---------- æ¨¡å¼ Aï¼šå…¨é‡é‡å»º ----------

def build_full_dataset(max_page: int = 200):
    """
    å¾ç¬¬ 1 é ä¸€è·¯çˆ¬åˆ° max_pageï¼ˆé‡åˆ°ç©ºé å°±æå‰åœï¼‰ï¼Œ
    æŠŠç›®å‰æ‰€æœ‰å€åŸŸå‹•æ…‹éƒ½æŠ“ä¸‹ä¾†ï¼Œå†èˆ‡ manual_gap åˆä½µï¼Œè¼¸å‡º mnd_pla.csvã€‚

    âœ” é€™ä¸€æ­¥å®Œå…¨ä¸è®€èˆŠ CSV â†’ å¯æŠŠ 109/09/17 ä¹‹å‰ç•™ä¸‹çš„äº‚ç¢¼æ•´å€‹æ´—æ‰ã€‚
    """
    print("ğŸš€ é–‹å§‹å…¨é‡é‡å»ºï¼ˆå¾ç¶²ç«™æŠ“åˆ°ç¾åœ¨æ‰€æœ‰è³‡æ–™ï¼‰")
    df = crawl_pages(max_page=max_page)
    print(f"ğŸŒ å¾ç¶²ç«™å…±æŠ“åˆ° {len(df)} ç­†")

    gap_df = load_manual_gap()
    final = merge_with_gap(df, gap_df)

    final.to_csv(DATA_PATH, index=False, encoding="utf-8-sig")
    print(f"âœ… å…¨é‡é‡å»ºå®Œæˆï¼Œå·²è¼¸å‡º {len(final)} ç­†åˆ° {DATA_PATH}")


# ---------- æ¨¡å¼ Bï¼šæ¯æ—¥å¢é‡æ›´æ–° ----------

def load_existing_data() -> pd.DataFrame:
    """è®€å…¥æ—¢æœ‰çš„ mnd_pla.csvï¼›è‹¥æ‰¾ä¸åˆ°å°±è‡ªå‹•è·‘ä¸€æ¬¡å…¨é‡é‡å»ºã€‚"""
    if not os.path.exists(DATA_PATH):
        print("âš ï¸ æ‰¾ä¸åˆ°æ—¢æœ‰ä¸»æª”ï¼Œå…ˆè·‘å…¨é‡é‡å»ºã€‚")
        build_full_dataset()
        return pd.read_csv(DATA_PATH, encoding="utf-8-sig")

    print(f"ğŸ“¥ è®€å–æ—¢æœ‰ä¸»æª”ï¼š{DATA_PATH}")
    df = pd.read_csv(DATA_PATH, encoding="utf-8-sig")
    print(f"  â†’ {len(df)} ç­†")
    return df


def daily_update(max_page: int = 3):
    """
    æ¯æ—¥æ›´æ–°ï¼š
    1. è®€æ—¢æœ‰ä¸»æª” mnd_pla.csv
    2. å»æŠ“æœ€è¿‘å¹¾é ï¼ˆé è¨­ 3 é ï¼‰çš„è³‡æ–™
    3. åªæŒ‘å‡ºã€Œurl ä¸åœ¨ä¸»æª”ã€çš„é‚£äº› â†’ è¦–ç‚ºæ–°è³‡æ–™
    4. append é€²ä¸»æª”ï¼Œå†èˆ‡ manual_gap åˆä½µï¼Œè¼¸å‡ºå› mnd_pla.csv
    """
    existing = load_existing_data()
    known_urls = set(existing.get("url", []))

    print("ğŸŒ æŠ“å–æœ€è¿‘å¹¾é ï¼ˆé è¨­ 3 é ï¼‰æ‰¾æ–°è³‡æ–™â€¦")
    recent_df = crawl_pages(max_page=max_page)
    if recent_df.empty:
        print("âš ï¸ æœ€è¿‘é é¢æ²’æœ‰æŠ“åˆ°ä»»ä½•è³‡æ–™ï¼ŒçµæŸã€‚")
        return

    is_new = ~recent_df["url"].isin(known_urls)
    new_rows = recent_df[is_new]
    print(f"ğŸ†• æ‰¾åˆ° {len(new_rows)} ç­†ã€Œä¸»æª”è£¡æ²’æœ‰çš„ã€æ–°è³‡æ–™")

    if new_rows.empty:
        print("âœ… æ²’æœ‰æ–°è³‡æ–™ï¼Œä¸»æª”ç¶­æŒä¸è®Šã€‚")
        return

    updated = pd.concat([existing, new_rows], ignore_index=True)
    gap_df = load_manual_gap()
    final = merge_with_gap(updated, gap_df)

    final.to_csv(DATA_PATH, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²å¯«å…¥æ–°è³‡æ–™ï¼Œç¾åœ¨å…±æœ‰ {len(final)} ç­†åˆ° {DATA_PATH}")


# ---------- å…¥å£é» ----------

def main():
    mode = os.getenv("MND_MODE", "").lower()
    if mode == "full":
        # ä¸€æ¬¡æ€§å…¨é‡ï¼šç¬¬ä¸€æ¬¡å»ºæª”ï¼Œæˆ–å“ªå¤©ä½ æƒ³é‡å»ºéƒ½å¯ä»¥å†è·‘
        build_full_dataset()
    else:
        # ä¾‹è¡Œï¼šæ¯å¤©æ’ç¨‹è·‘é€™å€‹
        daily_update()


if __name__ == "__main__":
    main()
