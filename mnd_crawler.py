#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
mnd_crawler.pyï¼ˆè¥¿å…ƒæ—¥æœŸï¼‹è£œä¸è¦†è“‹ç‰ˆï¼‰

åŠŸèƒ½ï¼š
- fullï¼šæŠŠç›®å‰ç¶²ç«™ä¸Šæ‰€æœ‰ã€Œå€åŸŸå‹•æ…‹ã€çš„å…±æ©Ÿ/æµ·åŸŸå…¬å‘ŠæŠ“ä¸‹ä¾† â†’ mnd_pla.csv
- dailyï¼šæ¯å¤©åªæŠ“æœ€æ–°ä¸€ç­†ï¼Œappend åˆ° mnd_pla.csv
- manual_gap.csvï¼šè£œä¸äº†çš„æ—¥æœŸç”¨é€™å€‹è£œï¼Œæœ€å¾Œæœƒä¸€èµ· merge é€² mnd_pla.csv

èª¿æ•´é‡é»ï¼š
1. åˆ—è¡¨é ä»ç”¨ https://www.mnd.gov.tw/news/plaactlist (+ /2, /3, ...)
2. å…§é ä»åªæŠ“ <div class="maincontent">
3. æŠŠæ°‘åœ‹æ—¥æœŸï¼ˆä¾‹å¦‚ 114.02.03ï¼‰è½‰æˆè¥¿å…ƒ YYYY-MM-DD
4. å¤šå­˜å…©æ¬„ï¼šæ¨™é¡Œã€ä¾†æºç¶²å€
5. manual_gap.csv è‡³å°‘è¦æœ‰ï¼šæ—¥æœŸ, å…§å®¹
   - æ—¥æœŸå¯å¯«ï¼š2025/2/3, 2025-02-03, 114/2/3, 114å¹´2æœˆ3æ—¥â€¦â€¦
   - æœƒçµ±ä¸€è½‰æˆè¥¿å…ƒ YYYY-MM-DD
   - è‹¥ç¼ºæ¨™é¡Œï¼ä¾†æºç¶²å€æœƒè£œé è¨­å€¼
6. è£œä¸è¦å‰‡ï¼š
   - ä»¥æ—¥æœŸç‚º key
   - å…ˆåˆªæ‰åŸå§‹è³‡æ–™ä¸­åŒæ—¥æœŸçš„åˆ—ï¼Œå†æŠŠè£œä¸åŠ é€²ä¾†ï¼ˆè£œä¸è¦†è“‹ï¼‰
"""

import sys
import time
import re
from pathlib import Path
from typing import List, Dict

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime


# ------------------------------------------------------------
# å¸¸æ•¸è¨­å®š
# ------------------------------------------------------------
BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"
HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"}

BASE_DIR = Path(__file__).parent
OUTPUT_CSV = BASE_DIR / "mnd_pla.csv"
MANUAL_GAP = BASE_DIR / "manual_gap.csv"

# ä½ åŸæœ¬ ASPX ç‰ˆæœ¬çš„é—œéµå­—ï¼ˆç¢ºå®šæŠ“å¾—åˆ°æ‰€æœ‰ç‰ˆæœ¬çš„å…±æ©Ÿå…¬å‘Šï¼‰
KEYWORDS = [
    "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
    "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
    "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "è¸°è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "é€¾è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
]


# ------------------------------------------------------------
# æ—¥æœŸå·¥å…·ï¼šå…¨éƒ¨è½‰æˆè¥¿å…ƒ YYYY-MM-DD
# ------------------------------------------------------------
def normalize_date_to_iso(date_str: str) -> str:
    """
    æŠŠè¼¸å…¥å­—ä¸²è½‰æˆè¥¿å…ƒ YYYY-MM-DD

    æ”¯æ´ç¯„ä¾‹ï¼š
    - 2025/2/3
    - 2025/02/03
    - 2025-02-03
    - 114/2/3ï¼ˆæ°‘åœ‹å¹´ â†’ è‡ªå‹• +1911ï¼‰
    - 114å¹´2æœˆ3æ—¥
    - 114.02.03ï¼ˆæœƒå…ˆè¢«å‘¼å«è€…è½‰æˆæ–œç·šï¼‰
    """
    if not isinstance(date_str, str):
        raise ValueError(f"æ—¥æœŸä¸æ˜¯å­—ä¸²: {date_str!r}")

    s = date_str.strip()
    if not s:
        raise ValueError("æ—¥æœŸæ˜¯ç©ºå­—ä¸²")

    # å¦‚æœæœ¬ä¾†å°±æ˜¯ YYYY-MM-DDï¼Œè©¦è‘— parse ä¸€ä¸‹ï¼ŒæˆåŠŸå°±ç›´æ¥å›å‚³
    try:
        dt = datetime.strptime(s, "%Y-%m-%d")
        return dt.strftime("%Y-%m-%d")
    except ValueError:
        pass

    # çµ±ä¸€æŠŠä¸­æ–‡å¹´æœˆæ—¥ã€é»ã€æ¸›è™Ÿéƒ½æ›æˆæ–œç·š
    s_clean = re.sub(r"[å¹´æœˆæ—¥.\-]", "/", s)
    s_clean = re.sub(r"/+", "/", s_clean).strip("/")

    parts = s_clean.split("/")
    if len(parts) != 3:
        raise ValueError(f"ç„¡æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str!r}ï¼ˆæ¸…æ´—å¾Œ: {s_clean!r}ï¼‰")

    y, m, d = parts
    y = y.strip()
    m = m.strip()
    d = d.strip()

    year = int(y)
    # ç²—ç•¥ï¼šå°æ–¼ 1911 è¦–ç‚ºæ°‘åœ‹å¹´
    if year < 1911:
        year = year + 1911

    month = int(m)
    day = int(d)

    dt = datetime(year, month, day)
    return dt.strftime("%Y-%m-%d")


def normalize_date_column(df: pd.DataFrame, col: str) -> pd.DataFrame:
    """
    æŠŠ df[col] å…¨éƒ¨è½‰æˆ YYYY-MM-DDï¼ˆå­—ä¸²ï¼‰ï¼Œå›å‚³æ–° DataFrame
    """
    if col not in df.columns:
        raise KeyError(f"DataFrame ä¸å«æ¬„ä½ {col!r}")
    df = df.copy()
    df[col] = df[col].astype(str).apply(normalize_date_to_iso)
    return df


# ------------------------------------------------------------
# GET with retry
# ------------------------------------------------------------
def safe_get(url: str, retries: int = 3, timeout: int = 20) -> str | None:
    """
    ç™¼ GETï¼Œå›å‚³æ­£ç¢ºè§£ç¢¼å¾Œçš„æ–‡å­—ï¼ˆå„ªå…ˆå˜—è©¦ utf-8 / big5 / cp950ï¼‰ï¼Œ
    é¿å…å°‘æ•¸èˆŠé é¢ï¼ˆä¾‹å¦‚ 109.09.17ï¼‰å‡ºç¾äº‚ç¢¼ã€‚
    """
    for i in range(1, retries + 1):
        try:
            r = requests.get(url, headers=HEADERS, timeout=timeout)
            r.raise_for_status()

            raw = r.content  # å…ˆæ‹¿ bytes

            # å„ªå…ˆæ”¾é€²å¯èƒ½çš„ç·¨ç¢¼
            enc_candidates: list[str] = []

            if r.encoding:
                enc_candidates.append(r.encoding)
            if r.apparent_encoding and r.apparent_encoding not in enc_candidates:
                enc_candidates.append(r.apparent_encoding)

            # å†è£œå¸¸è¦‹çš„
            for e in ("utf-8", "big5", "cp950"):
                if e not in enc_candidates:
                    enc_candidates.append(e)

            text = None
            for enc in enc_candidates:
                try:
                    text = raw.decode(enc)
                    # print(f"[DEBUG] {url} ä½¿ç”¨ç·¨ç¢¼ï¼š{enc}")
                    break
                except UnicodeDecodeError:
                    continue

            if text is None:
                # å…¨éƒ¨å¤±æ•—å°±ç”¨ utf-8 + replace æ’ä½
                text = raw.decode("utf-8", errors="replace")
                # print(f"[DEBUG] {url} ä½¿ç”¨ç·¨ç¢¼ï¼šutf-8 (replace)")

            return text

        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i} æ¬¡å¤±æ•—ï¼š{url} - {e}")
            time.sleep(1)

    print(f"âŒ æœ€çµ‚å¤±æ•—ï¼š{url}")
    return None

# ------------------------------------------------------------
# åˆ—è¡¨é 
# ------------------------------------------------------------
def build_list_url(page: int) -> str:
    # page=1: /plaactlist, page>=2: /plaactlist/2
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


def crawl_list_page(page: int) -> List[Dict]:
    """
    æŠ“æŸä¸€é åˆ—è¡¨ï¼Œåªç•™ä½ æŒ‡å®šé—œéµå­—çš„å…¬å‘Š
    å›å‚³ list[dict]:
        {
            "roc_date": "114.12.01",
            "url": "https://www.mnd.gov.tw/......",
            "title": "114.12.01ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹......"
        }
    """
    url = build_list_url(page)
    print(f"\nğŸ” æŠ“åˆ—è¡¨é ï¼š{url}")

    html = safe_get(url)
    if html is None:
        print("âš ï¸ åˆ—è¡¨é æŠ“å–å¤±æ•—ï¼Œè¦–ç‚ºç„¡è³‡æ–™")
        return []

    soup = BeautifulSoup(html, "html.parser")
    rows: List[Dict] = []

    for a in soup.find_all("a", href=True):
        title = a.get_text(strip=True)
        if not title:
            continue

        # é—œéµå­—éæ¿¾
        if not any(kw in title for kw in KEYWORDS):
            continue

        # ä¾‹å¦‚ï¼š114.12.01ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹é»é–±æ¬¡æ•¸ï¼š413 æ¬¡
        m = re.search(r"\d{3}\.\d{2}\.\d{2}", title)
        if not m:
            continue
        roc_date = m.group(0)

        article_url = requests.compat.urljoin(BASE_URL, a["href"])
        rows.append({"roc_date": roc_date, "url": article_url, "title": title})

    print(f"ğŸ“Œ æœ¬é æŠ“åˆ° {len(rows)} ç­†")
    return rows


# ------------------------------------------------------------
# å…§é ï¼šåªæŠ“ maincontent
# ------------------------------------------------------------
def extract_maincontent_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    main = soup.select_one("div.maincontent")
    if not main:
        return ""
    text = " ".join(main.stripped_strings)
    return text


def crawl_article(url: str) -> str:
    print(f"â¡ï¸ æŠ“æ–‡ç« é ï¼š{url}")
    html = safe_get(url)
    if html is None:
        return ""
    return extract_maincontent_text(html)


# ------------------------------------------------------------
# åˆä½µè£œä¸ manual_gap.csvï¼ˆä»¥è¥¿å…ƒæ—¥æœŸç‚º key è¦†è“‹ï¼‰
# ------------------------------------------------------------
def load_manual_gap() -> pd.DataFrame | None:
    """
    manual_gap.csv çµæ§‹ï¼ˆæœ€å°‘ï¼‰ï¼š
    æ—¥æœŸ,å…§å®¹
    2025/02/03,åœ‹é˜²éƒ¨ä»Šæ—¥è¡¨ç¤ºï¼Œâ€¦â€¦
    
    å¯é¸æ¬„ä½ï¼š
    - æ¨™é¡Œ
    - ä¾†æºç¶²å€

    æ—¥æœŸå¯ä»¥æ˜¯ï¼š
    - 2025/2/3, 2025-02-03
    - 114/2/3, 114å¹´2æœˆ3æ—¥, 114.02.03ï¼ˆæœƒè½‰æˆè¥¿å…ƒï¼‰

    å›å‚³çš„ DataFrameï¼š
    - æ—¥æœŸ å·²è½‰ç‚º YYYY-MM-DD
    - è‡³å°‘å«æœ‰ï¼šæ—¥æœŸ, æ¨™é¡Œ, å…§å®¹, ä¾†æºç¶²å€
    """
    if not MANUAL_GAP.exists():
        print("â„¹ï¸ æ‰¾ä¸åˆ° manual_gap.csvï¼Œç•¥éè£œä¸è®€å–")
        return None

    print(f"ğŸ“¥ è®€å–è£œä¸ï¼š{MANUAL_GAP}")
    gap = pd.read_csv(MANUAL_GAP, encoding="utf-8-sig")

    if "æ—¥æœŸ" not in gap.columns or "å…§å®¹" not in gap.columns:
        raise KeyError("manual_gap.csv è‡³å°‘è¦æœ‰ã€æ—¥æœŸã€ã€å…§å®¹ã€å…©å€‹æ¬„ä½ã€‚")

    # çµ±ä¸€æ—¥æœŸæ ¼å¼
    gap = normalize_date_column(gap, "æ—¥æœŸ")

    # è£œç¼ºæ¬„ä½
    if "æ¨™é¡Œ" not in gap.columns:
        gap["æ¨™é¡Œ"] = "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹"
    if "ä¾†æºç¶²å€" not in gap.columns:
        gap["ä¾†æºç¶²å€"] = ""

    # æ¬„ä½é †åºæ•´ç†
    gap = gap[["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]]

    print(f"ğŸ“¥ è£œä¸ç­†æ•¸ï¼š{len(gap)}")
    return gap


def apply_manual_gap(base_df: pd.DataFrame) -> pd.DataFrame:
    """
    ä»¥ã€Œæ—¥æœŸã€ï¼ˆYYYY-MM-DDï¼‰ç‚º key å¥—ç”¨è£œä¸ï¼š

    è¦å‰‡ï¼š
    1. è‹¥ manual_gap.csv ä¸å­˜åœ¨ â†’ ç›´æ¥å›å‚³ base_df
    2. è‹¥å­˜åœ¨ï¼š
       - è®€å‡º gap_dfï¼Œæ—¥æœŸè½‰æˆ YYYY-MM-DD
       - åˆªé™¤ base_df ä¸­æ‰€æœ‰ã€Œæ—¥æœŸåœ¨ gap_df è£¡ã€çš„åˆ—
       - å†æŠŠ gap_df åŠ é€²ä¾†
       - ä¾æ—¥æœŸæ’åºå¾Œå›å‚³
    """
    gap_df = load_manual_gap()
    if gap_df is None or gap_df.empty:
        print("â„¹ï¸ æ²’æœ‰è£œä¸æˆ–è£œä¸ç‚ºç©ºï¼Œç•¥éè£œä¸åˆä½µ")
        # ä»ç„¶ç¢ºä¿æ¬„ä½é½Šå…¨
        base_df = base_df.copy()
        for col in ["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]:
            if col not in base_df.columns:
                base_df[col] = ""
        base_df = base_df[["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]]
        # æ—¥æœŸå·²åœ¨å¤–é¢è™•ç†æˆ YYYY-MM-DDï¼Œé€™è£¡åªåšæ’åº
        base_df = base_df.sort_values("æ—¥æœŸ").reset_index(drop=True)
        return base_df

    base_df = base_df.copy()

    # ç¢ºä¿æ¬„ä½ä¸€è‡´
    for col in ["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]:
        if col not in base_df.columns:
            base_df[col] = ""

    base_df = base_df[["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]]
    gap_df = gap_df[["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]]

    # ä»¥æ—¥æœŸç‚º key è¦†è“‹ï¼šå…ˆåˆªå†åŠ 
    gap_dates = gap_df["æ—¥æœŸ"].unique().tolist()
    before_len = len(base_df)
    base_df = base_df[~base_df["æ—¥æœŸ"].isin(gap_dates)].reset_index(drop=True)
    after_len = len(base_df)
    print(f"ğŸ©¹ å¥—ç”¨è£œä¸ï¼šåˆªé™¤åŸæœ¬åŒæ—¥æœŸè³‡æ–™ {before_len - after_len} ç­†")

    merged_df = pd.concat([base_df, gap_df], ignore_index=True)

    # ä¾æ—¥æœŸæ’åºï¼ˆYYYY-MM-DD å­—ä¸²å¯ä»¥ç›´æ¥æ­£ç¢ºæ’åºï¼‰
    merged_df = merged_df.sort_values("æ—¥æœŸ").reset_index(drop=True)
    print(f"ğŸ©¹ å¥—ç”¨è£œä¸å¾Œç¸½ç­†æ•¸ï¼š{len(merged_df)}")
    return merged_df


# ------------------------------------------------------------
# fullï¼šæŠ“åˆ°ã€Œæ²’æœ‰æ–°æ–‡ç« ã€å°±è‡ªå‹•åœ
# ------------------------------------------------------------
def run_full():
    print("ğŸš€ [FULL] å…¨é‡æ¨¡å¼é–‹å§‹")

    all_rows: List[Dict] = []
    seen_urls: set[str] = set()
    page = 1
    consecutive_no_new = 0  # é€£çºŒå¹¾é ã€Œæ²’æœ‰æ–°æ–‡ç« ã€

    while True:
        entries = crawl_list_page(page)
        if not entries:
            print("âšª æ­¤é å®Œå…¨æ²’æœ‰ç¬¦åˆé—œéµå­—çš„æ–‡ç«  â†’ è¦–ç‚ºå°¾ç«¯ï¼Œåœæ­¢ã€‚")
            break

        # åªç•™ä¸‹æ²’çœ‹éçš„ url
        new_entries = [e for e in entries if e["url"] not in seen_urls]

        if not new_entries:
            consecutive_no_new += 1
            print(f"âšª ç¬¬ {page} é æ²’æœ‰æ–°æ–‡ç« ï¼ˆé€£çºŒ {consecutive_no_new} é ï¼‰ã€‚")

            # åœ‹é˜²éƒ¨åœ¨è¶…éæœ€å¾Œä¸€é æ™‚æœƒé‡è¤‡å›å‚³åŒä¸€é 
            # â†’ é€£çºŒå…©é éƒ½æ²’æœ‰æ–°ç¶²å€ï¼Œå°±è¦–ç‚ºå·²ç¶“åˆ·åˆ°æœ€å¾Œä¸€é 
            if consecutive_no_new >= 2:
                print("ğŸ”š é€£çºŒå…©é éƒ½æ²’æœ‰æ–°ç¶²å€ï¼Œåˆ¤å®šå·²åˆ°æœ€å¾Œä¸€é ï¼Œåœæ­¢å¾€å¾ŒæŠ“ã€‚")
                break
        else:
            consecutive_no_new = 0

        for e in new_entries:
            seen_urls.add(e["url"])
            content = crawl_article(e["url"])

            # è½‰æ—¥æœŸï¼š114.12.01 â†’ 114/12/01 â†’ 2025-12-01
            roc_date_slash = e["roc_date"].replace(".", "/")
            iso_date = normalize_date_to_iso(roc_date_slash)

            all_rows.append(
                {
                    "æ—¥æœŸ": iso_date,
                    "æ¨™é¡Œ": e["title"],
                    "å…§å®¹": content,
                    "ä¾†æºç¶²å€": e["url"],
                }
            )
            time.sleep(0.3)

        print(f"ğŸ“Š ç´¯ç©ç­†æ•¸ï¼š{len(all_rows)}")
        page += 1
        time.sleep(0.5)

        # å®‰å…¨ä¿éšªï¼šé˜²ç¦¦æ€§ä¸Šé™ï¼Œé¿å…æ„å¤– infinite loop
        if page > 1000:
            print("âš ï¸ é æ•¸è¶…é 1000ï¼Œå¼·åˆ¶åœæ­¢ï¼ˆæ‡‰è©²ä¸æœƒç™¼ç”Ÿï¼‰ã€‚")
            break

    df = pd.DataFrame(all_rows)

    if df.empty:
        print("âš ï¸ å…¨é‡çˆ¬å®Œçµæœç‚ºç©ºï¼Œè«‹æª¢æŸ¥åˆ—è¡¨ selector æˆ–ç¶²ç«™çµæ§‹ã€‚")
        # å»ºä¸€å€‹ç©ºçš„æ¨™æº–æ¬„ä½ CSVï¼Œè‡³å°‘ä¸æœƒç›´æ¥ç‚¸æ‰
        df = pd.DataFrame(columns=["æ—¥æœŸ", "å…¬å‘Šå…§å®¹"])
        df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
        print(f"âš ï¸ è¼¸å‡ºç©º CSVï¼š{OUTPUT_CSV}")
        return

    # æ—¥æœŸå·²ç¶“æ˜¯ ISOï¼Œé€™è£¡ä¸»è¦æ˜¯å¥—è£œä¸
    df = apply_manual_gap(df)

    # åªä¿ç•™ã€Œæ—¥æœŸã€ã€Œå…§å®¹ã€ï¼Œä¸¦æŠŠã€Œå…§å®¹ã€æ”¹åæˆã€Œå…¬å‘Šå…§å®¹ã€
    output_df = df[["æ—¥æœŸ", "å…§å®¹"]].rename(columns={"å…§å®¹": "å…¬å‘Šå…§å®¹"})

    # æ—¥æœŸç”±è¿‘åˆ°é æ’åºï¼ˆæ–°åˆ°èˆŠï¼‰
    output_df = output_df.sort_values("æ—¥æœŸ", ascending=False).reset_index(drop=True)

    output_df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
    print(f"âœ… å…¨é‡å®Œæˆï¼Œè¼¸å‡º {OUTPUT_CSV}ï¼Œå…± {len(output_df)} ç­†")

# ------------------------------------------------------------
# dailyï¼šæ¯å¤©åªæŠ“æœ€æ–°ä¸€ç­†
# ------------------------------------------------------------
def run_daily():
    print("ğŸ“… [DAILY] æ¯æ—¥æ¨¡å¼é–‹å§‹ï¼ˆåªæŠ“æœ€æ–°ä¸€ç­†ï¼‰")

    entries = crawl_list_page(1)
    if not entries:
        print("âš ï¸ ç¬¬ 1 é æŠ“ä¸åˆ°è³‡æ–™ï¼Œä»Šæ—¥ç•¥éã€‚")
        return

    newest = entries[0]

    roc_date_slash = newest["roc_date"].replace(".", "/")
    iso_date = normalize_date_to_iso(roc_date_slash)
    content = crawl_article(newest["url"])

    df_new = pd.DataFrame(
        [
            {
                "æ—¥æœŸ": iso_date,
                "æ¨™é¡Œ": newest["title"],
                "å…§å®¹": content,
                "ä¾†æºç¶²å€": newest["url"],
            }
        ]
    )

    if OUTPUT_CSV.exists():
        df_old = pd.read_csv(OUTPUT_CSV, encoding="utf-8-sig")

        # å¦‚æœèˆŠæª”æ˜¯ã€Œæ—¥æœŸ, å…¬å‘Šå…§å®¹ã€çš„æ ¼å¼ï¼Œå…ˆæ˜ å°„å›ã€Œå…§å®¹ã€
        if "å…§å®¹" not in df_old.columns and "å…¬å‘Šå…§å®¹" in df_old.columns:
            df_old["å…§å®¹"] = df_old["å…¬å‘Šå…§å®¹"]

        # èˆŠæª”å¦‚æœé‚„æ˜¯èˆŠæ ¼å¼ï¼ˆåªæœ‰æ—¥æœŸï¼‹å…§å®¹ã€æ°‘åœ‹æ—¥æœŸï¼‰ï¼Œé€™è£¡æœƒç¨å¾®ã€Œå¹«ä½ å‡ç´šã€ï¼š
        if "æ¨™é¡Œ" not in df_old.columns:
            df_old["æ¨™é¡Œ"] = ""
        if "ä¾†æºç¶²å€" not in df_old.columns:
            df_old["ä¾†æºç¶²å€"] = ""
        if df_old["æ—¥æœŸ"].astype(str).str.contains(r"/").any():
            # ç²—ç•¥åˆ¤æ–·èˆŠæª”å¯èƒ½æ˜¯æ°‘åœ‹æ ¼å¼ï¼ˆä¾‹å¦‚ 114/02/03ï¼‰
            try:
                df_old = normalize_date_column(df_old, "æ—¥æœŸ")
            except Exception as e:
                print(f"âš ï¸ èˆŠæª”æ—¥æœŸè½‰æ›å¤±æ•—ï¼š{e}")

        df = pd.concat([df_old, df_new], ignore_index=True)

    else:
        df = df_new

    # ä¸ç®¡æœ‰æ²’æœ‰èˆŠæª”ï¼Œéƒ½è¦åœ¨é€™è£¡å¥—ä¸€æ¬¡è£œä¸
    df = apply_manual_gap(df)

    # åªä¿ç•™ã€Œæ—¥æœŸã€ã€Œå…§å®¹ã€ï¼Œä¸¦æŠŠã€Œå…§å®¹ã€æ”¹åæˆã€Œå…¬å‘Šå…§å®¹ã€
    output_df = df[["æ—¥æœŸ", "å…§å®¹"]].rename(columns={"å…§å®¹": "å…¬å‘Šå…§å®¹"})

    # æ—¥æœŸç”±è¿‘åˆ°é æ’åºï¼ˆæ–°åˆ°èˆŠï¼‰
    output_df = output_df.sort_values("æ—¥æœŸ", ascending=False).reset_index(drop=True)

    output_df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

    print(f"âœ… æ¯æ—¥æ›´æ–°å®Œæˆï¼Œç¾åœ¨ {OUTPUT_CSV} å…± {len(output_df)} ç­†")

# ------------------------------------------------------------
# main
# ------------------------------------------------------------
if __name__ == "__main__":
    # python mnd_crawler.py full  â†’ å…¨é‡
    # python mnd_crawler.py       â†’ æ¯æ—¥æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "full":
        run_full()
    else:
        run_daily()
