# -*- coding: utf-8 -*-
"""mnd_crawler

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lhNZt1SQSxr7TwyDLd_xLSl4YUx2GGrs
"""

def extract_metrics(text):
    m_air = re.search(r"(å…±|è¨ˆ)\s*(\d+)\s*æ¶æ¬¡", text)
    aircraft_total = int(m_air.group(2)) if m_air else None

    m_adiz = re.search(r"å…¶ä¸­\s*(\d+)\s*æ¶æ¬¡.*?(ADIZ|ç©ºåŸŸ|ä¸­ç·š)", text)
    adiz_count = int(m_adiz.group(1)) if m_adiz else None

    m_ship = re.search(r"(å…±|è¨ˆ)\s*(\d+)\s*è‰¦", text)
    ship_count = int(m_ship.group(2)) if m_ship else None
    return {
        "åµæ¸¬åˆ°çš„å…±æ©Ÿç¸½æ•¸": aircraft_total,
        "é€²å…¥ADIZæˆ–è·¨è¶Šä¸­ç·š": adiz_count,
        "å…±è‰¦æ´»å‹•æ•¸é‡": ship_count,
    }

# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup #ç”¨ä¾†çˆ¬èŸ²ï¼Œ
import re
import pandas as pd
import time

BASE_URL = "https://www.mnd.gov.tw/PublishTable.aspx?Types=å³æ™‚è»äº‹å‹•æ…‹&title=åœ‹é˜²æ¶ˆæ¯"
HEADERS = {"User-Agent": "Mozilla/5.0"}

def parse_viewstate_fields(soup):
    def val(name):
        el = soup.find("input", {"name": name})
        return el["value"] if el and el.has_attr("value") else ""
    return {
        "__VIEWSTATE": val("__VIEWSTATE"),
        "__VIEWSTATEGENERATOR": val("__VIEWSTATEGENERATOR"),
        "__EVENTVALIDATION": val("__EVENTVALIDATION"),
    }

def extract_postback_target(a_tag):
    href = a_tag.get("href", "")
    m = re.search(r"__doPostBack\('([^']+)'", href)
    return m.group(1) if m else None

def parse_list_page(html):
    soup = BeautifulSoup(html, "html.parser")
    fields = parse_viewstate_fields(soup)
    items = []
    for tr in soup.select("table tr"):
        a = tr.find("a", href=True)
        if not a:
            continue
        title = a.get_text(strip=True)
        if "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹" not in title:
            continue
        target = extract_postback_target(a)
        date_text = None
        for td in tr.find_all("td"):
            if re.search(r"\d{3}/\d{1,2}/\d{1,2}", td.get_text()):
                date_text = td.get_text(strip=True)
                break
        items.append({"date": date_text, "target": target, "view": fields})
    return items

def fetch_detail(session, view_fields, target):
    data = {
        "__EVENTTARGET": target,
        "__EVENTARGUMENT": "",
        "__VIEWSTATE": view_fields["__VIEWSTATE"],
        "__VIEWSTATEGENERATOR": view_fields["__VIEWSTATEGENERATOR"],
        "__EVENTVALIDATION": view_fields["__EVENTVALIDATION"]
    }
    r = session.post(BASE_URL, headers=HEADERS, data=data, timeout=20)
    r.raise_for_status()
    return r.text

def extract_clean_paragraph(html):
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text(" ", strip=True)

    prefix = "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹"
    end_marker = "åœ‹è»é‹ç”¨ä»»å‹™æ©Ÿã€è‰¦åŠå²¸ç½®é£›å½ˆç³»çµ±åš´å¯†ç›£æ§èˆ‡æ‡‰è™•ã€‚"

    start = text.find(prefix)
    end = text.find(end_marker)

    if start != -1 and end != -1:
        segment = text[start:end + len(end_marker)]
    elif start != -1:
        segment = text[start:]
    else:
        segment = text

    # ğŸ”§ è™•ç†ã€Œæ¨™é¡Œé‡è¤‡ã€çš„æƒ…æ³ï¼š
    # å¦‚æœé–‹é ­é•·æˆã€Œä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹ ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹â€¦ã€
    double_prefix = prefix + " " + prefix
    if segment.startswith(double_prefix):
        segment = prefix + segment[len(double_prefix):]

    return segment
def crawl_all():
    session = requests.Session()
    page = 1
    records = []

    while True:
        url = f"{BASE_URL}&Page={page}"
        print(f"\næŠ“å–ç¬¬ {page} é : {url}")
        r = session.get(url, headers=HEADERS, timeout=20)
        if r.status_code != 200:
            print("ç„¡æ³•é€£ç·šï¼Œåœæ­¢ã€‚")
            break

        items = parse_list_page(r.text)
        if not items:
            print("æ²’æœ‰æ›´å¤šè³‡æ–™ï¼ŒçµæŸã€‚")
            break

        for i, it in enumerate(items, 1):
            print(f"({i}/{len(items)}) æŠ“å– {it['date']}")
            try:
                html_detail = fetch_detail(session, it["view"], it["target"])
                clean_text = extract_clean_paragraph(html_detail)
            except Exception as e:
                print("å…§é éŒ¯èª¤:", e)
                clean_text = ""
            metrics = extract_metrics(clean_text)

            record = {
                "æ—¥æœŸ": it["date"],
                "é€šå ±å…§å®¹": clean_text,
                **metrics
            }

            records.append(record)
            time.sleep(0.8)

        page += 1
        time.sleep(1.5)

    df = pd.DataFrame(records)
    return df

if __name__ == "__main__":
    df = crawl_all()
    df.to_csv("pla_daily_clean_full.csv", index=False, encoding="utf-8-sig")
    print("\nå…¨éƒ¨å®Œæˆï¼å…±æŠ“å–", len(df), "ç­†è³‡æ–™ã€‚")
    print(df.head(5))
