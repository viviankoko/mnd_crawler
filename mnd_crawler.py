#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
mnd_crawler.py  â€”  åœ‹é˜²éƒ¨ã€Œå€åŸŸå‹•æ…‹ã€çˆ¬èŸ²

åŠŸèƒ½ï¼š
1. full æ¨¡å¼ï¼ˆpython mnd_crawler.py fullï¼‰
   - å¾ /news/plaactlist é–‹å§‹ä¸€è·¯å¾€ä¸‹çˆ¬ï¼Œç›´åˆ°æŸé æ²’æœ‰ç¬¦åˆé—œéµå­—çš„é€£çµç‚ºæ­¢
   - æŠŠæ‰€æœ‰ç¬¦åˆé—œéµå­—çš„æ–‡ç« æ—¥æœŸï¼‹å…¨æ–‡æŠ“ä¸‹ä¾†
   - èˆ‡ manual_gap.csv åˆä½µå¾Œè¼¸å‡ºæˆ mnd_pla.csv

2. daily æ¨¡å¼ï¼ˆpython mnd_crawler.pyï¼‰
   - åªæŠ“ç¬¬ 1 é æœ€ä¸Šé¢ä¸€ç­†ï¼ˆå‡è¨­æ˜¯æœ€æ–°å…¬å‘Šï¼‰
   - append åˆ°æ—¢æœ‰çš„ mnd_pla.csvï¼Œå†èˆ‡ manual_gap.csv åˆä½µè¦†å¯« mnd_pla.csv

å…©å€‹æ¨¡å¼éƒ½æœƒï¼š
- ä¿ç•™ã€Œæ—¥æœŸ, å…§å®¹ã€å…©æ¬„
- ä»¥ã€Œæ—¥æœŸã€å»é‡ã€æ’åº
- è‹¥ manual_gap.csv å­˜åœ¨ï¼Œæœƒä¸€èµ·åˆä½µ

æ³¨æ„ï¼š
- manual_gap.csv å¯ä»¥æœ‰æ¨™é¡Œåˆ—ã€Œæ—¥æœŸ,å…§å®¹ã€ï¼Œä¹Ÿå¯ä»¥æ²’æœ‰ã€‚
"""

import sys
import time
import re
from pathlib import Path
from typing import List, Dict

import requests
from bs4 import BeautifulSoup
import pandas as pd

# ------------------------------------------------------------
# åŸºæœ¬è¨­å®š
# ------------------------------------------------------------
BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"
HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"}

BASE_DIR = Path(__file__).parent
OUTPUT_CSV = BASE_DIR / "mnd_pla.csv"
MANUAL_GAP = BASE_DIR / "manual_gap.csv"

# å’Œä½ èˆŠç‰ˆ ASPX çˆ¬èŸ²ä¸€æ¨£çš„æ¨™é¡Œé—œéµå­—ï¼ˆç¢ºä¿å„ç¨®ç‰ˆæœ¬éƒ½æŠ“å¾—åˆ°ï¼‰
KEYWORDS = [
    "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
    "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
    "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "è¸°è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "é€¾è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
]


# ------------------------------------------------------------
# å·¥å…·ï¼šGET with retry
# ------------------------------------------------------------
def safe_get(url: str, retries: int = 3, timeout: int = 20) -> str | None:
    for attempt in range(1, retries + 1):
        try:
            r = requests.get(url, headers=HEADERS, timeout=timeout)
            r.raise_for_status()
            r.encoding = r.apparent_encoding
            return r.text
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {attempt} æ¬¡å¤±æ•—ï¼š{url} - {e}")
            if attempt < retries:
                time.sleep(2)
    print(f"âŒ æ”¾æ£„æŠ“å–ï¼š{url}")
    return None


# ------------------------------------------------------------
# åˆ—è¡¨é ï¼špage=1 => /plaactlistï¼Œå…¶é¤˜ /plaactlist/2 ...
# ------------------------------------------------------------
def build_list_url(page: int) -> str:
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


def crawl_list_page(page: int) -> List[Dict]:
    """
    æŠ“æŸä¸€é åˆ—è¡¨ï¼Œå›å‚³ï¼š
      [{"roc_date": "114.12.03", "url": "https://..."}, ...]
    åªä¿ç•™æ¨™é¡Œå« KEYWORDS ä»»ä¸€å­—ä¸²çš„é …ç›®ã€‚
    """
    url = build_list_url(page)
    print(f"\nğŸ” æŠ“åˆ—è¡¨é ï¼š{url}")

    html = safe_get(url)
    if html is None:
        print("âš ï¸ åˆ—è¡¨é æŠ“å–å¤±æ•—ï¼Œè¦–ç‚ºæ²’æœ‰è³‡æ–™ã€‚")
        return []

    soup = BeautifulSoup(html, "html.parser")
    rows: List[Dict] = []

    for a in soup.find_all("a", href=True):
        text = a.get_text(strip=True)
        if not text:
            continue

        # æ¨™é¡Œå¿…é ˆåŒ…å«ä»»ä¸€é—œéµå­—
        if not any(kw in text for kw in KEYWORDS):
            continue

        # åˆ—è¡¨ä¸Šæœƒæœ‰é¡ä¼¼ã€Œ114.12.03ã€çš„æ—¥æœŸ
        m = re.search(r"\d{3}\.\d{2}\.\d{2}", text)
        if not m:
            continue
        roc_date = m.group(0)

        href = a["href"]
        article_url = requests.compat.urljoin(BASE_URL, href)

        rows.append({"roc_date": roc_date, "url": article_url})

    print(f"ğŸ“Œ æœ¬é æŠ“åˆ° {len(rows)} ç­†ï¼ˆç¬¦åˆé—œéµå­—ï¼‰")
    return rows


# ------------------------------------------------------------
# å…§é ï¼šåªæŠ“ <div class="maincontent"> çš„æ–‡å­—
# ------------------------------------------------------------
def extract_maincontent_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    main = soup.select_one("div.maincontent")
    if not main:
        return ""

    parts = list(main.stripped_strings)
    text = " ".join(parts)
    return text


def crawl_article(url: str) -> str:
    print(f"â¡ï¸ æŠ“æ–‡ç« é ï¼š{url}")
    html = safe_get(url)
    if html is None:
        return ""
    return extract_maincontent_text(html)


# ------------------------------------------------------------
# æ°‘åœ‹æ—¥æœŸæ’åº key
# ------------------------------------------------------------
def roc_sort_key(s: str):
    try:
        y, m, d = s.split("/")
        return int(y), int(m), int(d)
    except Exception:
        return (0, 0, 0)


# ------------------------------------------------------------
# åˆä½µè£œä¸ manual_gap.csv
# ------------------------------------------------------------
def load_manual_gap() -> pd.DataFrame:
    if not MANUAL_GAP.exists():
        print("â„¹ï¸ æ‰¾ä¸åˆ° manual_gap.csvï¼Œç•¥éè£œä¸ã€‚")
        return pd.DataFrame(columns=["æ—¥æœŸ", "å…§å®¹"])

    print(f"ğŸ“¥ è®€å–è£œä¸æª”ï¼š{MANUAL_GAP}")
    gap = pd.read_csv(MANUAL_GAP, encoding="utf-8-sig")

    # å…è¨±æœ‰æ¨™é¡Œåˆ—æˆ–æ²’æœ‰æ¨™é¡Œåˆ—
    if "æ—¥æœŸ" not in gap.columns or "å…§å®¹" not in gap.columns:
        # åªæ‹¿å‰å…©æ¬„ï¼Œæ”¹åæˆ æ—¥æœŸ / å…§å®¹
        cols = list(gap.columns)
        if len(cols) < 2:
            raise ValueError("manual_gap.csv è‡³å°‘éœ€è¦å…©æ¬„ï¼ˆæ—¥æœŸ, å…§å®¹ï¼‰")
        gap = gap.iloc[:, :2]
        gap.columns = ["æ—¥æœŸ", "å…§å®¹"]
    else:
        gap = gap[["æ—¥æœŸ", "å…§å®¹"]]

    return gap


def apply_manual_gap(df: pd.DataFrame) -> pd.DataFrame:
    gap = load_manual_gap()
    if not gap.empty:
        df = pd.concat([df, gap], ignore_index=True)

    if "æ—¥æœŸ" not in df.columns:
        raise ValueError("è³‡æ–™è¡¨ç¼ºå°‘ã€æ—¥æœŸã€æ¬„ä½ï¼Œç„¡æ³•æ’åºã€‚")

    df = df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
    df = df.sort_values("æ—¥æœŸ", key=lambda col: col.map(roc_sort_key))
    df = df.reset_index(drop=True)
    return df


# ------------------------------------------------------------
# æ¨¡å¼ä¸€ï¼šfull â€” å¾ç¬¬ 1 é ä¸€è·¯å¾€ä¸‹çˆ¬åˆ°ã€Œæ²’æœ‰è³‡æ–™ã€ç‚ºæ­¢
# ------------------------------------------------------------
def run_full():
    print("ğŸš€ [FULL] å…¨é‡æ¨¡å¼é–‹å§‹")
    all_rows: List[Dict] = []
    page = 1

    while True:
        entries = crawl_list_page(page)
        if not entries:
            print(f"âšª ç¬¬ {page} é æ²’æœ‰è³‡æ–™ï¼ˆæˆ–æŠ“å¤±æ•—ï¼‰ï¼Œåœæ­¢ã€‚")
            break

        for e in entries:
            date_str = e["roc_date"].replace(".", "/")
            content = crawl_article(e["url"])
            all_rows.append({"æ—¥æœŸ": date_str, "å…§å®¹": content})
            time.sleep(0.3)

        page += 1
        time.sleep(1.0)

    df = pd.DataFrame(all_rows)
    df = apply_manual_gap(df)

    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
    print(f"âœ… å…¨é‡å®Œæˆï¼Œå…± {len(df)} ç­†ï¼Œå·²å¯«å…¥ {OUTPUT_CSV.name}")


# ------------------------------------------------------------
# æ¨¡å¼äºŒï¼šdaily â€” åªæŠ“ç¬¬ 1 é çš„æœ€æ–°ä¸€ç­†
# ------------------------------------------------------------
def run_daily():
    print("ğŸ“… [DAILY] æ¯æ—¥æ¨¡å¼é–‹å§‹ï¼ˆåªæŠ“ç¬¬ 1 é æœ€ä¸Šé¢ä¸€ç­†ï¼‰")

    entries = crawl_list_page(1)
    if not entries:
        print("âš ï¸ ç¬¬ 1 é æ²’æœ‰ç¬¦åˆé—œéµå­—çš„è³‡æ–™ã€‚")
        return

    newest = entries[0]
    date_str = newest["roc_date"].replace(".", "/")
    content = crawl_article(newest["url"])

    df_new = pd.DataFrame([{"æ—¥æœŸ": date_str, "å…§å®¹": content}])

    if OUTPUT_CSV.exists():
        df_old = pd.read_csv(OUTPUT_CSV, encoding="utf-8-sig")
        df = pd.concat([df_old, df_new], ignore_index=True)
    else:
        df = df_new

    df = apply_manual_gap(df)
    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
    print(f"âœ… æ¯æ—¥æ›´æ–°å®Œæˆï¼Œç›®å‰å…± {len(df)} ç­†ï¼Œå·²å¯«å…¥ {OUTPUT_CSV.name}")


# ------------------------------------------------------------
# main
# ------------------------------------------------------------
if __name__ == "__main__":
    # python mnd_crawler.py full  => å…¨é‡
    # python mnd_crawler.py       => æ¯æ—¥
    if len(sys.argv) > 1 and sys.argv[1].lower() == "full":
        run_full()
    else:
        run_daily()
