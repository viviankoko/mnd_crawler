import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path
import re
import pandas as pd
import time

BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"
}

BASE_DIR = Path(__file__).parent

MANUAL_CSV = BASE_DIR / "manual_gap.csv"
LATEST_CSV = BASE_DIR / "pla_daily_latest.csv"
FINAL_CSV = BASE_DIR / "pla_daily_clean_full.csv"

# åƒè€ƒèˆŠç‰ˆ ASP.NET çˆ¬èŸ²çš„é—œéµå­—åˆ—è¡¨ï¼Œç•¥å¾®æ“´å……
TITLE_KEYWORDS = [
    "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
    "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
    "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "è¸°è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "é€¾è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
    "è‡ºæµ·å‘¨é‚Šç©ºåŸŸæƒ…å‹¢å‹•æ…‹æ–°èç¨¿",
    "è‡ºæµ·å‘¨é‚Šç©ºåŸŸæƒ…å‹¢å‹•æ…‹",
]


# ------------------------------------------------------------
# HTTP with retry
# ------------------------------------------------------------
def safe_get(url: str, max_retries: int = 3, timeout: int = 20) -> str | None:
    for attempt in range(1, max_retries + 1):
        try:
            r = requests.get(url, headers=HEADERS, timeout=timeout)
            r.raise_for_status()
            r.encoding = r.apparent_encoding
            return r.text
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {attempt} æ¬¡æŠ“å–å¤±æ•—ï¼š{url} - {e}")
            if attempt == max_retries:
                print(f"âŒ æ”¾æ£„æŠ“å–ï¼š{url}")
                return None
            time.sleep(2)


# ------------------------------------------------------------
# æ—¥æœŸå·¥å…·
# ------------------------------------------------------------
def normalize_date_str(s: str) -> str:
    """
    çµ±ä¸€æˆæ°‘åœ‹å¹´ï¼šYYY/MM/DD
    - æ°‘åœ‹ï¼š114/12/3 -> 114/12/03
    - è¥¿å…ƒï¼š2025/2/3 -> 114/02/03
    å…¶ä»–å¥‡æ€ªæ ¼å¼å°±åŸæ¨£ä¸Ÿå›å»
    """
    s = str(s).strip()
    if not s:
        return s

    m_roc = re.match(r"^(\d{3})/(\d{1,2})/(\d{1,2})$", s)
    if m_roc:
        y = int(m_roc.group(1))
        m = int(m_roc.group(2))
        d = int(m_roc.group(3))
        return f"{y:03d}/{m:02d}/{d:02d}"

    m_ad = re.match(r"^(\d{4})/(\d{1,2})/(\d{1,2})$", s)
    if m_ad:
        y_ad = int(m_ad.group(1))
        m = int(m_ad.group(2))
        d = int(m_ad.group(3))
        y_roc = y_ad - 1911
        return f"{y_roc:03d}/{m:02d}/{d:02d}"

    return s


def roc_to_sort_key(s: str):
    try:
        y, m, d = s.split("/")
        return int(y), int(m), int(d)
    except Exception:
        return (0, 0, 0)


# ------------------------------------------------------------
# åˆ—è¡¨é 
# ------------------------------------------------------------
def build_list_url(page: int) -> str:
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


def crawl_list_page(page: int):
    """
    å›å‚³ Noneï¼šæ•´é  timeout / 503 ä¹‹é¡
    å›å‚³ []ï¼šæœ‰æŠ“åˆ°é é¢ï¼Œä½†æ²’æœ‰ä»»ä½•ç¬¦åˆé—œéµå­—çš„é …ç›®
    å›å‚³ list[{roc_date, url}]
    """
    url = build_list_url(page)
    print(f"\nğŸ” æŠ“åˆ—è¡¨é ï¼š{url}")

    html = safe_get(url)
    if html is None:
        return None

    soup = BeautifulSoup(html, "html.parser")
    rows = []

    for a in soup.find_all("a", href=True):
        text = a.get_text(strip=True)

        # æ¨™é¡Œä¸­æœ‰ä»»ä¸€ keyword æ‰æŠ“
        if not any(kw in text for kw in TITLE_KEYWORDS):
            continue

        # ä¾‹å¦‚ï¼š111.11.08 11æœˆ8æ—¥è‡ºæµ·å‘¨é‚Šç©ºåŸŸæƒ…å‹¢å‹•æ…‹æ–°èç¨¿ é»é–±æ¬¡æ•¸â€¦
        m = re.search(r"\d{3}\.\d{2}\.\d{2}", text)
        if not m:
            continue

        roc_date = m.group(0)  # 111.11.08
        href = a["href"]
        article_url = urljoin(BASE_URL, href)

        rows.append(
            {
                "roc_date": roc_date,
                "url": article_url,
            }
        )

    print(f"ğŸ“Œ æœ¬é æŠ“åˆ° {len(rows)} ç­†")
    return rows


# ------------------------------------------------------------
# å…§æ–‡
# ------------------------------------------------------------
def extract_maincontent_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    main_div = soup.select_one("div.maincontent")
    if main_div is None:
        return ""
    parts = list(main_div.stripped_strings)
    return " ".join(parts)


def crawl_article_text(url: str) -> str:
    print(f"â¡ï¸ æŠ“æ–‡ç« é ï¼š{url}")
    html = safe_get(url)
    if html is None:
        return ""
    return extract_maincontent_text(html)


# ------------------------------------------------------------
# ä¸»æµç¨‹ï¼šçˆ¬åˆ° list/189
# ------------------------------------------------------------
def crawl_all_pages(max_pages: int = 189) -> pd.DataFrame:
    data_rows = []

    for page in range(1, max_pages + 1):
        entries = crawl_list_page(page)

        # æ•´é çˆ†æ‰ â†’ ç•¥éç¹¼çºŒ
        if entries is None:
            print(f"âšª ç¬¬ {page} é æŠ“å–å¤±æ•—ï¼Œç•¥éã€‚")
            continue

        # æœ‰é é¢ä½†å‰›å¥½æ²’æœ‰ç¬¦åˆ keyword çš„æ¢ç›® â†’ ä¹Ÿç¹¼çºŒ
        if len(entries) == 0:
            print(f"âšª ç¬¬ {page} é æ²’æœ‰ç¬¦åˆé—œéµå­—çš„å…¬å‘Šã€‚")
            continue

        for entry in entries:
            text = crawl_article_text(entry["url"])
            date_str = entry["roc_date"].replace(".", "/")
            date_str = normalize_date_str(date_str)

            data_rows.append(
                {
                    "æ—¥æœŸ": date_str,
                    "å…§å®¹": text,
                }
            )

    return pd.DataFrame(data_rows)


# ------------------------------------------------------------
# åˆä½µ manual_gap.csv
# ------------------------------------------------------------
def merge_with_manual(df_new: pd.DataFrame) -> pd.DataFrame:
    if MANUAL_CSV.exists():
        print(f"ğŸ“¥ è®€å–æ‰‹å‹•è£œé½Šæª”æ¡ˆï¼š{MANUAL_CSV}")
        df_raw = pd.read_csv(MANUAL_CSV, dtype=str)

        if {"æ—¥æœŸ", "å…§å®¹"}.issubset(df_raw.columns):
            df_manual = df_raw[["æ—¥æœŸ", "å…§å®¹"]].copy()
        else:
            df_manual = pd.read_csv(
                MANUAL_CSV, header=None, names=["æ—¥æœŸ", "å…§å®¹"], dtype=str
            )
    else:
        print("âš ï¸ æ‰¾ä¸åˆ° manual_gap.csvï¼Œåªä½¿ç”¨æœ¬æ¬¡çˆ¬åˆ°çš„è³‡æ–™ã€‚")
        df_manual = pd.DataFrame(columns=["æ—¥æœŸ", "å…§å®¹"])

    if not df_manual.empty:
        df_manual["æ—¥æœŸ"] = df_manual["æ—¥æœŸ"].astype(str).map(normalize_date_str)
    if not df_new.empty:
        df_new["æ—¥æœŸ"] = df_new["æ—¥æœŸ"].astype(str).map(normalize_date_str)

    df_manual = df_manual.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")
    df_new = df_new.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")

    df_all = pd.concat([df_manual, df_new], ignore_index=True)
    df_all = df_all.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")
    df_all = df_all.sort_values(by="æ—¥æœŸ", key=lambda col: col.map(roc_to_sort_key))

    return df_all


# ------------------------------------------------------------
# main
# ------------------------------------------------------------
def main():
    print("ğŸš€ é–‹å§‹çˆ¬å–åœ‹é˜²éƒ¨å€åŸŸå‹•æ…‹â€¦")

    df_new = crawl_all_pages()
    print(f"\nâœ… æœ¬æ¬¡å…±çˆ¬åˆ° {len(df_new)} ç­†è³‡æ–™")

    if not df_new.empty:
        # é€™æ¬¡æ–°çˆ¬åˆ°çš„åŸå§‹è³‡æ–™ï¼ˆæœ‰æ¬„ä½åç¨±ï¼‰
        df_new.to_csv(LATEST_CSV, index=False, encoding="utf-8-sig")
        print(f"ğŸ“ å·²å¯«å…¥æœ€æ–°çˆ¬å–è³‡æ–™ï¼š{LATEST_CSV}")

    df_final = merge_with_manual(df_new)
    df_final.to_csv(FINAL_CSV, index=False, encoding="utf-8-sig")

    print(f"ğŸ å·²å¯«å…¥æœ€çµ‚å®Œæ•´è³‡æ–™ï¼š{FINAL_CSV}")
    print(f"ğŸ“Š æœ€çµ‚ç­†æ•¸ï¼š{len(df_final)}")


if __name__ == "__main__":
    main()
