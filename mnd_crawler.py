#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import pandas as pd
import time
import sys
from pathlib import Path

BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"
HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"}

BASE_DIR = Path(__file__).parent
OUTPUT_CSV = BASE_DIR / "mnd_pla.csv"
MANUAL_GAP = BASE_DIR / "manual_gap.csv"


# ------------------------------------------------------------
# GET with retry
# ------------------------------------------------------------
def safe_get(url: str, retries=3):
    for i in range(retries):
        try:
            r = requests.get(url, headers=HEADERS, timeout=20)
            r.raise_for_status()
            r.encoding = r.apparent_encoding
            return r.text
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡å¤±æ•—ï¼š{url} - {e}")
            time.sleep(1)
    print(f"âŒ æœ€çµ‚å¤±æ•—ï¼š{url}")
    return None


# ------------------------------------------------------------
# åˆ—è¡¨é  URLï¼ˆä¿ç•™ä½ çš„åŽŸç‰ˆè¦å‰‡ï¼‰
# ------------------------------------------------------------
def build_list_url(page: int):
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


# ------------------------------------------------------------
# æŠ“åˆ—è¡¨é ï¼ˆæ²¿ç”¨ä½ åŽŸå§‹æ¢ä»¶ï¼ŒåªæŠ“ç‰¹å®šæ¨™é¡Œï¼‰
# ------------------------------------------------------------
def crawl_list_page(page: int):
    url = build_list_url(page)
    print(f"\nðŸ” æŠ“åˆ—è¡¨é ï¼š{url}")

    html = safe_get(url)
    if html is None:
        return []

    soup = BeautifulSoup(html, "html.parser")
    rows = []

    for a in soup.find_all("a", href=True):
        text = a.get_text(strip=True)

        # âœ” ä¿ç•™ä½ æŒ‡å®šçš„æ–‡ç« æ¨™é¡Œ
        if "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹" not in text:
            continue

        # æŠ“æ—¥æœŸï¼šä¾‹å¦‚ 114.12.03
        m = re.search(r"\d{3}\.\d{2}\.\d{2}", text)
        if not m:
            continue
        roc_date = m.group(0)

        article_url = urljoin(BASE_URL, a["href"])
        rows.append({"roc_date": roc_date, "url": article_url})

    print(f"ðŸ“Œ æœ¬é æŠ“åˆ° {len(rows)} ç­†")
    return rows


# ------------------------------------------------------------
# æ–‡ç« å…§æ–‡ï¼šåªæŠ“ maincontentã€æ¸…é™¤äº‚ç¢¼
# ------------------------------------------------------------
def extract_maincontent_text(html: str):
    soup = BeautifulSoup(html, "html.parser")
    main = soup.select_one("div.maincontent")
    if not main:
        return ""

    text = " ".join(main.stripped_strings)

    # âœ” åµæ¸¬é¡žä¼¼ 109/09/17 é‚£ç¨®ä¿„æ–‡å­—äº‚ç¢¼ï¼ˆä½ èªªè£œä¸æ˜¯è£œåˆ¥çš„å€é–“ä¹Ÿæ²’é—œä¿‚ï¼‰
    if re.search(r"[Ð°-ÑÐ-Ð¯Ñ‘Ð]+", text):
        print("âš ï¸ åµæ¸¬åˆ°äº‚ç¢¼ â†’ äº¤çµ¦è£œä¸è™•ç†")
        return ""

    return text


def crawl_article(url: str):
    print(f"âž¡ï¸ æŠ“æ–‡ç« é ï¼š{url}")
    html = safe_get(url)
    if html is None:
        return ""
    return extract_maincontent_text(html)


# ------------------------------------------------------------
# æ°‘åœ‹æ—¥æœŸæŽ’åº
# ------------------------------------------------------------
def roc_sort_key(s: str):
    try:
        y, m, d = s.split("/")
        return int(y), int(m), int(d)
    except:
        return (0, 0, 0)


# ------------------------------------------------------------
# åˆä½µè£œä¸ manual_gap.csv
# ------------------------------------------------------------
def apply_manual_gap(df: pd.DataFrame):
    if MANUAL_GAP.exists():
        print(f"ðŸ“¥ åˆä½µè£œä¸ï¼š{MANUAL_GAP}")
        # å‡è¨­ manual_gap.csv æ²’æœ‰æ¬„ä½åç¨±ã€å…©æ¬„ï¼šæ—¥æœŸ,å…§å®¹
        gap = pd.read_csv(
            MANUAL_GAP,
            encoding="utf-8-sig",
            header=None,
            names=["æ—¥æœŸ", "å…§å®¹"],
        )
        df = pd.concat([df, gap], ignore_index=True)

    # ä»¥ã€Œæ—¥æœŸã€åŽ»é‡ï¼Œè£œä¸åœ¨å¾Œé¢ â†’ è£œä¸å„ªå…ˆ
    if "æ—¥æœŸ" in df.columns:
        df = df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
        df = df.sort_values("æ—¥æœŸ", key=lambda col: col.map(roc_sort_key))

    return df


# ------------------------------------------------------------
# å…¨é‡æ¨¡å¼ï¼šä¸€æ¬¡çˆ¬æ‰€æœ‰é é¢
# ------------------------------------------------------------
def run_full():
    print("ðŸš€ [FULL] å…¨é‡æ¨¡å¼é–‹å§‹")
    all_rows = []

    for page in range(1, 300):
        entries = crawl_list_page(page)
        if not entries:
            break

        for e in entries:
            content = crawl_article(e["url"])
            date_str = e["roc_date"].replace(".", "/")
            all_rows.append({"æ—¥æœŸ": date_str, "å…§å®¹": content})
            time.sleep(0.3)

    df = pd.DataFrame(all_rows)
    df = apply_manual_gap(df)
    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

    print(f"âœ… å…¨é‡å®Œæˆï¼Œå…± {len(df)} ç­†")


# ------------------------------------------------------------
# æ¯æ—¥æ¨¡å¼ï¼šåªæŠ“æœ€æ–°ä¸€ç­†ï¼ˆç¬¬ 1 é ç¬¬ä¸€ç­†ï¼‰
# ------------------------------------------------------------
def run_daily():
    print("ðŸ“… [DAILY] æ¯æ—¥æ¨¡å¼é–‹å§‹ï¼ˆåªæŠ“æœ€æ–°ä¸€ç­†ï¼‰")

    entries = crawl_list_page(1)
    if not entries:
        print("âš ï¸ ç„¡è³‡æ–™")
        return

    newest = entries[0]
    date_str = newest["roc_date"].replace(".", "/")
    content = crawl_article(newest["url"])

    df_new = pd.DataFrame([{"æ—¥æœŸ": date_str, "å…§å®¹": content}])

    if OUTPUT_CSV.exists():
        df_old = pd.read_csv(OUTPUT_CSV, encoding="utf-8-sig")
        df = pd.concat([df_old, df_new], ignore_index=True)
    else:
        df = df_new

    df = apply_manual_gap(df)
    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

    print(f"âœ… å·²æ›´æ–°ï¼Œå…± {len[df]} ç­†")


# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------
if __name__ == "__main__":
    # python mnd_crawler.py full  â†’ å…¨é‡
    # python mnd_crawler.py       â†’ æ¯æ—¥åªæŠ“æœ€æ–°
    if len(sys.argv) > 1 and sys.argv[1] == "full":
        run_full()
    else:
        run_daily()
