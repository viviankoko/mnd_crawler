# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import time

BASE_URL = "https://www.mnd.gov.tw/PublishTable.aspx?Types=å³æ™‚è»äº‹å‹•æ…‹&title=åœ‹é˜²æ¶ˆæ¯"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# ---------------------------------------------------------
# è§£æè»æ©Ÿ/è»è‰¦æ•¸é‡ï¼ˆå¯ä¿ç•™ï¼Œä¸å½±éŸ¿ä½ ç›®å‰åªç”¢å‡ºæ—¥æœŸï¼‹å…¨æ–‡ï¼‰
# ---------------------------------------------------------
def extract_metrics(text):
    m_air = re.search(r"(å…±|è¨ˆ)\s*(\d+)\s*æ¶æ¬¡", text)
    aircraft_total = int(m_air.group(2)) if m_air else None

    m_adiz = re.search(r"å…¶ä¸­\s*(\d+)\s*æ¶æ¬¡.*?(ADIZ|ç©ºåŸŸ|ä¸­ç·š)", text)
    adiz_count = int(m_adiz.group(1)) if m_adiz else None

    m_ship = re.search(r"(å…±|è¨ˆ)\s*(\d+)\s*è‰¦", text)
    ship_count = int(m_ship.group(2)) if m_ship else None

    return {
        "åµæ¸¬åˆ°çš„å…±æ©Ÿç¸½æ•¸": aircraft_total,
        "é€²å…¥ADIZæˆ–è·¨è¶Šä¸­ç·š": adiz_count,
        "å…±è‰¦æ´»å‹•æ•¸é‡": ship_count,
    }


# ---------------------------------------------------------
# ASP.NET ViewState
# ---------------------------------------------------------
def parse_viewstate_fields(soup):
    def val(name):
        el = soup.find("input", {"name": name})
        return el["value"] if el and el.has_attr("value") else ""
    return {
        "__VIEWSTATE": val("__VIEWSTATE"),
        "__VIEWSTATEGENERATOR": val("__VIEWSTATEGENERATOR"),
        "__EVENTVALIDATION": val("__EVENTVALIDATION"),
    }


def extract_postback_target(a_tag):
    href = a_tag.get("href", "")
    m = re.search(r"__doPostBack\('([^']+)'", href)
    return m.group(1) if m else None


# ---------------------------------------------------------
# åˆ—è¡¨é ï¼šæŠ“æ—¥æœŸèˆ‡ postback TARGET
# ---------------------------------------------------------
def parse_list_page(html):
    soup = BeautifulSoup(html, "html.parser")
    fields = parse_viewstate_fields(soup)
    items = []

    KEYWORDS = [
        "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
        "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
        "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
        "è¸°è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
        "é€¾è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
        "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
        "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
        "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
    ]

    for tr in soup.select("table tr"):
        a = tr.find("a", href=True)
        if not a:
            continue
        title = a.get_text(strip=True)

        if not any(kw in title for kw in KEYWORDS):
            continue

        target = extract_postback_target(a)

        date_text = None
        for td in tr.find_all("td"):
            if re.search(r"\d{3}/\d{1,2}/\d{1,2}", td.get_text()):
                date_text = td.get_text(strip=True)
                break

        items.append({"date": date_text, "target": target, "view": fields})

    return items


# ---------------------------------------------------------
# å…§é è«‹æ±‚ï¼ˆåŠ  retryï¼‰
# ---------------------------------------------------------
def fetch_detail(session, view_fields, target, retries=2):
    data = {
        "__EVENTTARGET": target,
        "__EVENTARGUMENT": "",
        "__VIEWSTATE": view_fields["__VIEWSTATE"],
        "__VIEWSTATEGENERATOR": view_fields["__VIEWSTATEGENERATOR"],
        "__EVENTVALIDATION": view_fields["__EVENTVALIDATION"],
    }

    for attempt in range(retries):
        try:
            r = session.post(BASE_URL, headers=HEADERS, data=data, timeout=40)
            r.raise_for_status()
            return r.text
        except requests.exceptions.ReadTimeout:
            print(f"å…§é é€¾æ™‚ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰ï¼Œé‡è©¦ä¸­â€¦")
            time.sleep(2)

    print("å…§é è®€å–å¤±æ•—ï¼Œç•¥éæ­¤ç­†è³‡æ–™ã€‚")
    return ""


# ---------------------------------------------------------
# èƒå–å…¬å‘Šå…¨æ–‡ï¼ˆä¸é‡è¤‡æ¨™é¡Œï¼‹æ”¯æ´æ‰€æœ‰çµå°¾ï¼‰
# ---------------------------------------------------------
def extract_clean_paragraph(html):
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text(" ", strip=True)

    PREFIXES = [
        "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
        "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
        "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
        "è¸°è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
        "é€¾è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
        "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
        "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
        "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
    ]

    # æ‰¾æœ€æ—©å‡ºç¾çš„ prefix
    start = -1
    used_prefix = None
    for p in PREFIXES:
        pos = text.find(p)
        if pos != -1 and (start == -1 or pos < start):
            start = pos
            used_prefix = p

    if start == -1:
        return None

    END_PHRASES = [
        "ä¸‹è¼‰å°ˆå€",
        "åœ‹è»é‹ç”¨ä»»å‹™æ©Ÿã€è‰¦åŠå²¸ç½®é£›å½ˆç³»çµ±åš´å¯†ç›£æ§èˆ‡æ‡‰è™•ã€‚",
        "åœ‹è»é‹ç”¨ä»»å‹™æ©Ÿã€è‰¦åŠå²¸ç½®é£›å½ˆç³»çµ±åš´å¯†ç›£æ§èˆ‡æ‡‰è™•",
    ]

    end_candidates = []
    for phrase in END_PHRASES:
        pos = text.find(phrase, start)
        if pos != -1:
            if "æ‡‰è™•" in phrase:
                end_candidates.append(pos + len(phrase))
            else:
                end_candidates.append(pos)

    if end_candidates:
        end = min(end_candidates)
    else:
        end = len(text)

    segment = text[start:end]

    # å»æ‰æ¨™é¡Œé‡è¤‡
    if used_prefix:
        dup = used_prefix + " " + used_prefix
        if segment.startswith(dup):
            segment = used_prefix + segment[len(dup):]

    return segment.strip()


# ---------------------------------------------------------
# çˆ¬å…¨éƒ¨è³‡æ–™ï¼ˆæ­£å¼ç‰ˆæœ¬ï¼‰
# ---------------------------------------------------------
def crawl_all():
    session = requests.Session()
    page = 1
    records = []

    while True:
        url = f"{BASE_URL}&Page={page}"
        print(f"\næŠ“å–ç¬¬ {page} é : {url}")

        # åˆ—è¡¨é  retry
        try:
            r = session.get(url, headers=HEADERS, timeout=40)
        except requests.exceptions.ReadTimeout:
            print(f"ç¬¬ {page} é é€¾æ™‚ï¼Œå†è©¦ä¸€æ¬¡â€¦")
            time.sleep(2)
            continue

        if r.status_code != 200:
            print("ç„¡æ³•é€£ç·š")
            break

        items = parse_list_page(r.text)
        if not items:
            print("å·²ç„¡æ›´å¤šè³‡æ–™ã€‚")
            break

        for it in items:
            print(f"â¡ æŠ“å– {it['date']}")

            html_detail = fetch_detail(session, it["view"], it["target"])
            clean_text = extract_clean_paragraph(html_detail)

            records.append({
                "æ—¥æœŸ": it["date"],
                "é€šå ±å…§å®¹": clean_text,
            })

            time.sleep(0.8)

        page += 1
        time.sleep(1.5)

    return pd.DataFrame(records)


# ---------------------------------------------------------
# Debugï¼šåªæŠ“æŸä¸€å¤©
# ---------------------------------------------------------
def debug_one_day(DEBUG_DATE):
    session = requests.Session()
    page = 1

    while True:
        url = f"{BASE_URL}&Page={page}"
        print(f"æŸ¥é  {page} â€¦ {url}")

        try:
            r = session.get(url, headers=HEADERS, timeout=40)
        except requests.exceptions.ReadTimeout:
            print(f"ç¬¬ {page} é é€¾æ™‚ï¼Œå†è©¦ä¸€æ¬¡â€¦")
            time.sleep(2)
            continue

        items = parse_list_page(r.text)
        if not items:
            print("æ‰¾ä¸åˆ°é€™ä¸€å¤©ã€‚")
            return

        for it in items:
            if it["date"] == DEBUG_DATE:
                print(f"\nğŸ¯ æ‰¾åˆ°æ—¥æœŸï¼š{DEBUG_DATE}")
                html_detail = fetch_detail(session, it["view"], it["target"])
                clean_text = extract_clean_paragraph(html_detail)

                print("\n=== HTML detail (å‰ 1200 å­—) ===")
                print(html_detail[:1200])
                print("\n=== clean_text ===")
                print(clean_text)
                return

        page += 1


# ---------------------------------------------------------
# main
# ---------------------------------------------------------
if __name__ == "__main__":
    df = crawl_all()
    df.to_csv("pla_daily_clean_full.csv", index=False, encoding="utf-8-sig")
    print("\nå…¨éƒ¨å®Œæˆï¼ç­†æ•¸ =", len(df))
