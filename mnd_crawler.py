#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
mnd_crawler.py â€” æœ€çµ‚ç©©å®šç‰ˆï¼ˆä¾ç…§ä½ çš„æŒ‡å®šç¿»é æ ¼å¼ï¼‰
---------------------------------------------------
åˆ—è¡¨é  URL æ ¼å¼å®Œå…¨ä¾ä½ è¦æ±‚ï¼š

p=1: https://www.mnd.gov.tw/news/plaactlist/
p=2: https://www.mnd.gov.tw/news/plaactlist/2
p=3: https://www.mnd.gov.tw/news/plaactlist/3
...

åŒæ™‚ä¿®æ­£ href è§£æèˆ‡å…§é å®Œæ•´ URL çµ„æ³•ï¼Œé¿å…å‡ºç¾ www.mnd.gov.twnewsã€‚
"""

import os
import time
import re
from typing import List, Dict
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
import pandas as pd

BASE = "https://www.mnd.gov.tw"
LIST_URL = BASE + "/news/plaactlist/"

DATA_PATH = "mnd_pla.csv"
GAP_PATH = "manual_gap.csv"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0 Safari/537.36"
    )
}

# -----------------------------------------------------
# æŠ“å–å·¥å…·
# -----------------------------------------------------

def fetch(url: str, session=None) -> str:
    s = session or requests.Session()
    r = s.get(url, headers=HEADERS, timeout=20)
    r.encoding = "utf-8"
    return r.text


def parse_list_page(html: str) -> List[Dict]:
    """è§£æåˆ—è¡¨é ï¼šæŠ“å‡º date / title / url"""
    soup = BeautifulSoup(html, "lxml")
    records: List[Dict] = []

    # æŠ“æ‰€æœ‰å«æœ‰ plaact/ çš„é€£çµï¼ˆå¯èƒ½æ˜¯ news/plaact/... æˆ– /news/plaact/...ï¼‰
    for a in soup.select("a[href*='plaact/']"):
        href = a.get("href") or ""

        # â˜…â˜…â˜… æ­£ç¢º URL çµ„æ³• â€” æ°¸é ä¸æœƒå†è®Šæˆ www.mnd.gov.twnews â˜…â˜…â˜…
        url = urljoin(BASE + "/", href)

        # å¾çˆ¶å±¤æ‰¾æ—¥æœŸ
        row = a.find_parent("tr") or a.find_parent("div")
        date_str = ""
        if row:
            m = re.search(r"\d{3}[./]\d{2}[./]\d{2}", row.get_text())
            if m:
                date_str = m.group(0).replace(".", "/")

        records.append({
            "date": date_str,
            "title": a.get_text(strip=True),
            "url": url,
        })

    return records


def parse_article(html: str) -> Dict[str, str]:
    """è§£æå…§é ï¼šæŠ“ content åŠå…§é æ—¥æœŸ"""
    soup = BeautifulSoup(html, "lxml")

    main = soup.select_one(".maincontent")
    content_text = main.get_text("\n", strip=True) if main else ""

    date_str = ""
    pageinfo = soup.select_one(".pageinfo")
    if pageinfo:
        spans = pageinfo.select("span")
        if len(spans) >= 2:
            raw = spans[1].get_text(strip=True)
            m = re.search(r"\d{3}[./]\d{2}[./]\d{2}", raw)
            if m:
                date_str = m.group(0).replace(".", "/")

    return {"date": date_str, "content": content_text}


# -----------------------------------------------------
# çˆ¬å¤šé 
# -----------------------------------------------------

def crawl_pages(max_page: int) -> pd.DataFrame:
    all_rows: List[Dict] = []
    session = requests.Session()

    for page in range(1, max_page + 1):

        # â˜…â˜…â˜… ç…§ä½ æŒ‡å®šçš„ç¿»é æ ¼å¼ â˜…â˜…â˜…
        if page == 1:
            list_url = LIST_URL      # å¿…é ˆçµå°¾æœ‰ "/"
        else:
            list_url = LIST_URL.rstrip("/") + f"/{page}"

        print(f"ğŸ” æŠ“åˆ—è¡¨é ï¼š{list_url}")

        try:
            html = fetch(list_url, session=session)
        except Exception as e:
            print(f"âš ï¸ åˆ—è¡¨é æŠ“å–å¤±æ•— {list_url}: {e}")
            continue

        base_records = parse_list_page(html)
        if not base_records:
            print(f"é  {page} æ²’æŠ“åˆ°ä»»ä½• plaact é€£çµï¼Œåœæ­¢ã€‚")
            break

        print(f"é  {page} æŠ“åˆ° {len(base_records)} ç­†")

        # æŠ“æ¯å‰‡å…§é 
        for rec in base_records:
            art_url = rec["url"]
            try:
                art_html = fetch(art_url, session=session)
            except Exception as e:
                print(f"âš ï¸ å…§é æŠ“å–å¤±æ•— {art_url}: {e}")
                continue

            art = parse_article(art_html)

            all_rows.append({
                "date": art["date"] or rec["date"],
                "title": rec["title"],
                "url": rec["url"],
                "content": art["content"],
            })

            time.sleep(0.3)

    df = pd.DataFrame(all_rows)

    if not df.empty:
        df = df.drop_duplicates(subset=["url"], keep="last").reset_index(drop=True)

    return df


# -----------------------------------------------------
# manual_gap
# -----------------------------------------------------

def load_manual_gap() -> pd.DataFrame:
    if not os.path.exists(GAP_PATH):
        print("ğŸ” manual_gap.csv ä¸å­˜åœ¨ï¼Œç•¥éã€‚")
        return pd.DataFrame()

    df = pd.read_csv(GAP_PATH, encoding="utf-8-sig")
    print(f"ğŸ“¥ è®€å–è£œä¸ï¼Œå…± {len(df)} ç­†")
    return df


def merge_with_gap(main_df, gap_df):
    if gap_df.empty:
        return main_df.reset_index(drop=True)

    merged = pd.concat([main_df, gap_df], ignore_index=True)
    merged = merged.drop_duplicates(subset=["url"], keep="last")
    merged = merged.sort_values("date").reset_index(drop=True)
    return merged


# -----------------------------------------------------
# å…¨é‡
# -----------------------------------------------------

def build_full_dataset(max_page: int = 200):
    print("ğŸš€ å…¨é‡é‡å»ºé–‹å§‹")
    df = crawl_pages(max_page=max_page)
    print(f"ğŸŒ å…±æŠ“åˆ° {len(df)} ç­†")

    gap = load_manual_gap()
    final = merge_with_gap(df, gap)

    final.to_csv(DATA_PATH, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²è¼¸å‡º {len(final)} ç­†è‡³ {DATA_PATH}")


# -----------------------------------------------------
# daily
# -----------------------------------------------------

def load_existing_data():
    if not os.path.exists(DATA_PATH):
        print("âš ï¸ æ‰¾ä¸åˆ°ä¸»æª”ï¼Œæ”¹è·‘å…¨é‡ã€‚")
        build_full_dataset()
        return pd.read_csv(DATA_PATH, encoding="utf-8-sig")

    df = pd.read_csv(DATA_PATH, encoding="utf-8-sig")
    print(f"ğŸ“¥ ä¸»æª” {len(df)} ç­†")
    return df


def daily_update(max_page: int = 3):
    existing = load_existing_data()
    known = set(existing["url"].tolist())

    print("ğŸŒ æŠ“å–æœ€è¿‘å¹¾é æ‰¾æ–°è³‡æ–™")
    recent = crawl_pages(max_page=max_page)

    is_new = ~recent["url"].isin(known)
    new_rows = recent[is_new]
    print(f"ğŸ†• æ–°å¢ {len(new_rows)} ç­†")

    updated = pd.concat([existing, new_rows], ignore_index=True)
    gap = load_manual_gap()
    final = merge_with_gap(updated, gap)

    final.to_csv(DATA_PATH, index=False, encoding="utf-8-sig")
    print(f"âœ… å¯«å…¥å®Œç•¢ï¼Œç›®å‰ {len(final)} ç­†")


# -----------------------------------------------------
# main
# -----------------------------------------------------

def main():
    mode = os.getenv("MND_MODE", "").lower()

    if mode == "full":
        build_full_dataset()
    else:
        daily_update()


if __name__ == "__main__":
    main()
