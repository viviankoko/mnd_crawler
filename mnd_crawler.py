import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path
import re
import pandas as pd
import time

BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"
}

BASE_DIR = Path(__file__).parent

# è·¯å¾‘ï¼ˆéƒ½åœ¨ repo æ ¹ç›®éŒ„ï¼‰
MANUAL_CSV = BASE_DIR / "manual_gap.csv"
LATEST_CSV = BASE_DIR / "pla_daily_latest.csv"
FINAL_CSV = BASE_DIR / "pla_daily_clean_full.csv"


# ------------------------------------------------------------
# å·¥å…·ï¼šRetry åŒ…è£ï¼ˆåˆ—è¡¨é ã€æ–‡ç« é å…±ç”¨ï¼‰
# ------------------------------------------------------------
def safe_get(url: str, max_retries: int = 3, timeout: int = 20):
    """ä»¥ retry æ–¹å¼æŠ“å–é é¢ï¼Œå¤±æ•—æœƒå›å‚³ Noneï¼ˆä¸è®“ç¨‹å¼ crashï¼‰"""
    for attempt in range(1, max_retries + 1):
        try:
            r = requests.get(url, headers=HEADERS, timeout=timeout)
            r.raise_for_status()
            r.encoding = r.apparent_encoding
            return r.text
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {attempt} æ¬¡æŠ“å–å¤±æ•—ï¼š{url} - {e}")
            if attempt == max_retries:
                print(f"âŒ æ”¾æ£„æŠ“å–ï¼ˆæœ€çµ‚å¤±æ•—ï¼‰ï¼š{url}")
                return None
            time.sleep(2)


# ------------------------------------------------------------
# åˆ—è¡¨é ï¼ˆæœ‰ retryã€é˜² timeoutã€é˜² 503ã€ä¸è®“ workflow å´©ï¼‰
# ------------------------------------------------------------
def build_list_url(page: int) -> str:
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


def crawl_list_page(page: int):
    url = build_list_url(page)
    print(f"\nğŸ” æŠ“åˆ—è¡¨é ï¼š{url}")

    html = safe_get(url)
    if html is None:
        print(f"âš ï¸ åˆ—è¡¨é å¤±æ•—ï¼Œè¦–ç‚ºç„¡è³‡æ–™ â†’ åœæ­¢æŠ“å–å¾ŒçºŒé é¢")
        return []  # è®“ crawl_all_pages() åœæ­¢

    soup = BeautifulSoup(html, "html.parser")
    rows = []

    for a in soup.find_all("a"):
        text = a.get_text(strip=True)
        if "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹" not in text:
            continue

        m = re.search(r"\d{3}\.\d{2}\.\d{2}", text)
        if not m:
            continue

        roc_date = m.group(0)
        href = a.get("href")
        if not href:
            continue

        article_url = urljoin(BASE_URL, href)

        rows.append(
            {
                "roc_date": roc_date,
                "url": article_url,
            }
        )

    print(f"ğŸ“Œ æœ¬é æŠ“åˆ° {len(rows)} ç­†")
    return rows


# ------------------------------------------------------------
# æ–‡ç« é æ“·å–ï¼ˆæœ‰ retryã€é˜²å™ªéŸ³ï¼‰
# ------------------------------------------------------------
def extract_maincontent_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    main_div = soup.select_one("div.maincontent")
    if main_div is None:
        return ""

    parts = list(main_div.stripped_strings)
    return " ".join(parts)


def crawl_article_text(url: str) -> str:
    print(f"â¡ï¸ æŠ“æ–‡ç« é ï¼š{url}")

    html = safe_get(url)
    if html is None:
        return ""

    return extract_maincontent_text(html)


# ------------------------------------------------------------
# æ—¥æœŸæ’åºå·¥å…·ï¼ˆæ°‘åœ‹å¹´ï¼‰
# ------------------------------------------------------------
def roc_to_sort_key(s: str):
    try:
        y, m, d = s.split("/")
        return int(y), int(m), int(d)
    except Exception:
        return (0, 0, 0)


# ------------------------------------------------------------
# ä¸»æµç¨‹ï¼šæŠ“æ‰€æœ‰é é¢
# ------------------------------------------------------------
def crawl_all_pages(max_pages: int = 200) -> pd.DataFrame:
    data_rows = []

    for page in range(1, max_pages + 1):
        entries = crawl_list_page(page)

        if not entries:
            print(f"âšª ç¬¬ {page} é ç„¡è³‡æ–™ï¼ŒçµæŸæŠ“å–ã€‚")
            break

        for entry in entries:
            text = crawl_article_text(entry["url"])
            date_str = entry["roc_date"].replace(".", "/")

            data_rows.append(
                {
                    "æ—¥æœŸ": date_str,
                    "å…§å®¹": text,
                }
            )

    return pd.DataFrame(data_rows)


# ------------------------------------------------------------
# åˆä½µ manual_gap.csv
# ------------------------------------------------------------
def merge_with_manual(df_new: pd.DataFrame) -> pd.DataFrame:
    if MANUAL_CSV.exists():
        print(f"ğŸ“¥ è®€å–æ‰‹å‹•è£œé½Šæª”æ¡ˆï¼š{MANUAL_CSV}")
        df_manual = pd.read_csv(MANUAL_CSV, header=None, names=["æ—¥æœŸ", "å…§å®¹"])
    else:
        print("âš ï¸ æœªæ‰¾åˆ° manual_gap.csv")
        df_manual = pd.DataFrame(columns=["æ—¥æœŸ", "å…§å®¹"])

    df_manual = df_manual.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")
    df_new = df_new.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")

    df_all = pd.concat([df_manual, df_new], ignore_index=True)
    df_all = df_all.drop_duplicates(subset=["æ—¥æœŸ"], keep="first")
    df_all = df_all.sort_values(by="æ—¥æœŸ", key=lambda col: col.map(roc_to_sort_key))

    return df_all


# ------------------------------------------------------------
# main()
# ------------------------------------------------------------
def main():
    print("ğŸš€ é–‹å§‹çˆ¬å–åœ‹é˜²éƒ¨å€åŸŸå‹•æ…‹â€¦")

    df_new = crawl_all_pages()
    print(f"\nâœ… æœ¬æ¬¡å…±çˆ¬åˆ° {len(df_new)} ç­†è³‡æ–™")

    if len(df_new) > 0:
        df_new.to_csv(LATEST_CSV, index=False, header=False, encoding="utf-8-sig")
        print(f"ğŸ“ å·²å¯«å…¥æœ€æ–°çˆ¬å–è³‡æ–™ï¼š{LATEST_CSV}")

    df_final = merge_with_manual(df_new)
    df_final.to_csv(FINAL_CSV, index=False, header=False, encoding="utf-8-sig")

    print(f"ğŸ å·²å¯«å…¥æœ€çµ‚å®Œæ•´è³‡æ–™ï¼š{FINAL_CSV}")
    print(f"ğŸ“Š æœ€çµ‚ç­†æ•¸ï¼š{len(df_final)}")


if __name__ == "__main__":
    main()
