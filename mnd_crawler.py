#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æœ€çµ‚ç‰ˆ mnd_crawler.pyï¼ˆä½ ç›´æ¥è²¼é€™ä»½å°±èƒ½ç”¨ï¼‰

âœ” ä½¿ç”¨ä½ æä¾›çš„ã€å·²ç¢ºèªå¯çˆ¬åˆ°æ‰€æœ‰è³‡æ–™çš„ã€ŒèˆŠ ASP.NET æ¶æ§‹ã€çˆ¬èŸ²
âœ” æ—¥æœŸçµ±ä¸€è½‰æˆè¥¿å…ƒ YYYY-MM-DD
âœ” å¥—ç”¨ manual_gap.csv è£œä¸ï¼ˆå®Œå…¨è¦†è“‹åŒä¸€å¤©ï¼‰
âœ” è£œä¸åˆä½µå¾ŒæŒ‰æ—¥æœŸç”±æ–°â†’èˆŠæ’åº
âœ” CSV ä¸åŠ å¼•è™Ÿã€ä¸æ›è¡Œ
âœ” è‡ªå‹•åµæ¸¬æœ€å¾Œä¸€é ï¼Œä¸æœƒæŠ“åˆ° 126 ä¹‹å¾Œäº‚è·‘
"""

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import time
from pathlib import Path
from datetime import datetime


# -------------------------
# è·¯å¾‘è¨­å®š
# -------------------------
BASE_DIR = Path(__file__).parent
OUTPUT_CSV = BASE_DIR / "mnd_pla_air_sea.csv"
MANUAL_GAP = BASE_DIR / "manual_gap.csv"

BASE_URL = "https://www.mnd.gov.tw/PublishTable.aspx?Types=å³æ™‚è»äº‹å‹•æ…‹&title=åœ‹é˜²æ¶ˆæ¯"
HEADERS = {"User-Agent": "Mozilla/5.0"}


# -------------------------
# æ°‘åœ‹/è¥¿å…ƒæ—¥æœŸè™•ç†
# -------------------------
def normalize_date(date_str: str) -> str:
    """
    æ¥å—æ ¼å¼ï¼š
    - 2025/2/3
    - 114/9/23ï¼ˆæ°‘åœ‹ â†’ +1911ï¼‰
    - 109.09.17
    - 2025-02-03
    æœ€çµ‚è¼¸å‡º YYYY-MM-DD
    """
    if not isinstance(date_str, str):
        return ""

    s = date_str.strip()
    s = re.sub(r"[å¹´æœˆæ—¥.\-]", "/", s)
    s = re.sub(r"/+", "/", s).strip("/")

    parts = s.split("/")
    if len(parts) != 3:
        return ""

    y, m, d = parts
    y = int(y)
    if y < 1911:  # æ°‘åœ‹å¹´
        y += 1911

    m = int(m)
    d = int(d)
    return f"{y:04d}-{m:02d}-{d:02d}"


# -------------------------
# èˆŠ ASP.NET ViewStateï¼ˆä½ çš„ç¨‹å¼ç¢¼åŸå°ä¿ç•™ï¼‰
# -------------------------
def extract_viewstate_fields(soup):
    def val(name):
        el = soup.find("input", {"name": name})
        return el["value"] if el and el.has_attr("value") else ""
    return {
        "__VIEWSTATE": val("__VIEWSTATE"),
        "__VIEWSTATEGENERATOR": val("__VIEWSTATEGENERATOR"),
        "__EVENTVALIDATION": val("__EVENTVALIDATION"),
    }


def extract_postback_target(a_tag):
    m = re.search(r"__doPostBack\('([^']+)'", a_tag.get("href", ""))
    return m.group(1) if m else None


# -------------------------
# è§£æåˆ—è¡¨é ï¼ˆä½ çš„ç‰ˆæœ¬ï¼Œ100%ç…§æ¬ï¼‰
# -------------------------
def parse_list_page(html):
    soup = BeautifulSoup(html, "html.parser")
    fields = extract_viewstate_fields(soup)
    items = []

    KEYWORDS = [
        "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
        "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
        "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
        "è¸°è¶Šæµ·å³½ä¸­ç·š",
        "é€¾è¶Šæµ·å³½ä¸­ç·š",
        "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
        "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
        "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
    ]

    for tr in soup.select("table tr"):
        a = tr.find("a", href=True)
        if not a:
            continue

        title = a.get_text(strip=True)
        if not any(kw in title for kw in KEYWORDS):
            continue

        target = extract_postback_target(a)

        date_text = ""
        for td in tr.find_all("td"):
            if re.search(r"\d{3}[./]\d{1,2}[./]\d{1,2}", td.get_text()):
                date_text = td.get_text(strip=True)
                break

        items.append({"date": date_text, "target": target, "view": fields})

    return items


# -------------------------
# å…§é  AJAX PostBackï¼ˆä½ çš„ç‰ˆæœ¬ï¼Œç…§æ¬ï¼‰
# -------------------------
def fetch_detail(session, view_fields, target):
    data = {
        "__EVENTTARGET": target,
        "__EVENTARGUMENT": "",
        "__VIEWSTATE": view_fields["__VIEWSTATE"],
        "__VIEWSTATEGENERATOR": view_fields["__VIEWSTATEGENERATOR"],
        "__EVENTVALIDATION": view_fields["__EVENTVALIDATION"],
    }

    for _ in range(3):
        try:
            r = session.post(BASE_URL, headers=HEADERS, data=data, timeout=40)
            r.raise_for_status()
            return r.text
        except Exception:
            time.sleep(2)

    return ""


# -------------------------
# æŠ½å–å…§æ–‡ï¼ˆä½ çš„ç‰ˆæœ¬ï¼‰
# -------------------------
def extract_clean_paragraph(html):
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text(" ", strip=True)

    PREFIX_LIST = [
        "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
        "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
        "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
        "è¸°è¶Šæµ·å³½ä¸­ç·š",
        "é€¾è¶Šæµ·å³½ä¸­ç·š",
        "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
        "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
        "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
    ]

    start = min([text.find(p) for p in PREFIX_LIST if p in text] or [-1])
    if start == -1:
        return text

    # åœåœ¨ã€Œåœ‹è»é‹ç”¨ä»»å‹™æ©Ÿã€è‰¦åŠå²¸ç½®é£›å½ˆç³»çµ±åš´å¯†ç›£æ§èˆ‡æ‡‰è™•ã€‚ã€
    end_candidates = []
    END_PHRASES = [
        "åœ‹è»é‹ç”¨ä»»å‹™æ©Ÿã€è‰¦åŠå²¸ç½®é£›å½ˆç³»çµ±åš´å¯†ç›£æ§èˆ‡æ‡‰è™•ã€‚",
        "åœ‹è»é‹ç”¨ä»»å‹™æ©Ÿã€è‰¦åŠå²¸ç½®é£›å½ˆç³»çµ±åš´å¯†ç›£æ§èˆ‡æ‡‰è™•",
    ]

    for ph in END_PHRASES:
        pos = text.find(ph, start)
        if pos != -1:
            end_candidates.append(pos + len(ph))

    end = min(end_candidates) if end_candidates else len(text)
    seg = text[start:end]
    return seg.replace("\n", " ").replace("\r", " ").strip()


# -------------------------
# ä½¿ç”¨ä½ çš„çˆ¬æ³•å¾€ä¸‹ç¿»é ï¼ˆç›´åˆ°çœŸçš„æ²’è³‡æ–™ï¼‰
# -------------------------
def crawl_all():
    session = requests.Session()
    page = 1
    records = []

    while True:
        url = f"{BASE_URL}&Page={page}"
        print(f"ğŸ“„ æŠ“å–ç¬¬ {page} é ...")

        try:
            r = session.get(url, headers=HEADERS, timeout=40)
        except:
            print("é€¾æ™‚ï¼Œå†è©¦ä¸€æ¬¡...")
            time.sleep(2)
            continue

        items = parse_list_page(r.text)

        if not items:
            print(f"ğŸ”¥ ç¬¬ {page} é æŠ“ä¸åˆ°è³‡æ–™ â†’ è¦–ç‚ºæœ€å¾Œä¸€é ï¼Œåœæ­¢")
            break

        for it in items:
            html_detail = fetch_detail(session, it["view"], it["target"])
            clean_text = extract_clean_paragraph(html_detail)
            date_norm = normalize_date(it["date"])

            records.append({
                "æ—¥æœŸ": date_norm,
                "æ¨™é¡Œ": "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
                "å…§å®¹": clean_text,
                "ä¾†æºç¶²å€": BASE_URL,
            })

            time.sleep(0.6)

        page += 1

    return pd.DataFrame(records)


# -------------------------
# è£œä¸åŠŸèƒ½ï¼ˆè¦†è“‹åŒä¸€å¤©ï¼‰
# -------------------------
def load_manual_gap():
    if not MANUAL_GAP.exists():
        return None

    df = pd.read_csv(MANUAL_GAP)

    if "æ—¥æœŸ" not in df.columns or "å…§å®¹" not in df.columns:
        raise ValueError("manual_gap.csv å¿…é ˆæœ‰ã€æ—¥æœŸã€ã€å…§å®¹ã€å…©æ¬„")

    df["æ—¥æœŸ"] = df["æ—¥æœŸ"].apply(normalize_date)

    if "æ¨™é¡Œ" not in df.columns:
        df["æ¨™é¡Œ"] = "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹"
    if "ä¾†æºç¶²å€" not in df.columns:
        df["ä¾†æºç¶²å€"] = ""

    return df[["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]]


def apply_gap(df, gap):
    if gap is None:
        return df

    df = df.copy()
    gap_dates = gap["æ—¥æœŸ"].unique().tolist()

    df = df[~df["æ—¥æœŸ"].isin(gap_dates)]
    merged = pd.concat([df, gap], ignore_index=True)

    return merged.sort_values("æ—¥æœŸ", ascending=False).reset_index(drop=True)


# -------------------------
# main
# -------------------------
def main():
    print("ğŸš€ é–‹å§‹çˆ¬å–è³‡æ–™")
    df = crawl_all()

    print(f"âœ” çˆ¬åˆ°å…± {len(df)} ç­†")

    gap = load_manual_gap()
    if gap is not None:
        print(f"âœ” è¼‰å…¥è£œä¸ {len(gap)} ç­†")

    final = apply_gap(df, gap)

    print(f"âœ” å¥—ç”¨è£œä¸å¾Œå…± {len(final)} ç­†")

    # å¯¦éš› CSVï¼ˆç„¡å¼•è™Ÿï¼Œæ•´æ®µæ–‡å­—ä¸æ›è¡Œï¼‰
    final.to_csv(
        OUTPUT_CSV,
        index=False,
        encoding="utf-8-sig",
        quoting=3  # csv.QUOTE_NONE
    )

    print(f"ğŸ‰ å·²è¼¸å‡º CSV â†’ {OUTPUT_CSV}")


if __name__ == "__main__":
    main()
