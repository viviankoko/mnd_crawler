import sys
import time
import re
from pathlib import Path
from typing import List, Dict

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime



BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"
HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"}

BASE_DIR = Path(__file__).parent
OUTPUT_CSV = BASE_DIR / "mnd_pla.csv"
MANUAL_GAP = BASE_DIR / "manual_gap.csv"

# åœ‹é˜²éƒ¨æ‰€æœ‰å…¬å‘Šç‰ˆæœ¬çš„é—œéµå­—
KEYWORDS = [
    "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
    "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
    "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "è¸°è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "é€¾è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
]

#æ—¥æœŸè™•ç†
def normalize_date_to_iso(date_str: str) -> str:

    if not isinstance(date_str, str):
        raise ValueError(f"æ—¥æœŸä¸æ˜¯å­—ä¸²: {date_str!r}")

    s = date_str.strip()
    if not s:
        raise ValueError("æ—¥æœŸæ˜¯ç©ºå­—ä¸²")

    # å¦‚æœæœ¬ä¾†å°±æ˜¯ YYYY-MM-DDï¼Œè©¦è‘— parse ä¸€ä¸‹ï¼ŒæˆåŠŸå°±ç›´æ¥å›å‚³
    try:
        dt = datetime.strptime(s, "%Y-%m-%d")
        return dt.strftime("%Y-%m-%d")
    except ValueError:
        pass

    
    s_clean = re.sub(r"[å¹´æœˆæ—¥.\-]", "/", s)
    s_clean = re.sub(r"/+", "/", s_clean).strip("/")

    parts = s_clean.split("/")
    if len(parts) != 3:
        raise ValueError(f"ç„¡æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str!r}ï¼ˆæ¸…æ´—å¾Œ: {s_clean!r}ï¼‰")

    y, m, d = parts
    y = y.strip()
    m = m.strip()
    d = d.strip()

    year = int(y)
   
    if year < 1911:
        year = year + 1911

    month = int(m)
    day = int(d)

    dt = datetime(year, month, day)
    return dt.strftime("%Y-%m-%d")


def normalize_date_column(df: pd.DataFrame, col: str) -> pd.DataFrame:
    """
    æŠŠ df[col] å…¨éƒ¨è½‰æˆ YYYY-MM-DDï¼ˆå­—ä¸²ï¼‰ï¼Œå›å‚³æ–° DataFrame
    """
    if col not in df.columns:
        raise KeyError(f"DataFrame ä¸å«æ¬„ä½ {col!r}")
    df = df.copy()
    df[col] = df[col].astype(str).apply(normalize_date_to_iso)
    return df



def safe_get(url: str, retries: int = 3, timeout: int = 20) -> str | None:
   
    for i in range(1, retries + 1):
        try:
            r = requests.get(url, headers=HEADERS, timeout=timeout)
            r.raise_for_status()

            raw = r.content 

            
            enc_candidates: list[str] = []

            if r.encoding:
                enc_candidates.append(r.encoding)
            if r.apparent_encoding and r.apparent_encoding not in enc_candidates:
                enc_candidates.append(r.apparent_encoding)

            
            for e in ("utf-8", "big5", "cp950"):
                if e not in enc_candidates:
                    enc_candidates.append(e)

            text = None
            for enc in enc_candidates:
                try:
                    text = raw.decode(enc)
             
                    break
                except UnicodeDecodeError:
                    continue

            if text is None:
    
                text = raw.decode("utf-8", errors="replace")
     

            return text

        except Exception as e:
            print(f"ç¬¬ {i} æ¬¡å¤±æ•—ï¼š{url} - {e}")
            time.sleep(1)

    print(f"æœ€çµ‚å¤±æ•—ï¼š{url}")
    return None


#åˆ—è¡¨é 

def build_list_url(page: int) -> str:
    # page=1: /plaactlist, page>=2: /plaactlist/2
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


def crawl_list_page(page: int) -> List[Dict]:
    """
    æŠ“æŸä¸€é åˆ—è¡¨ï¼Œåªç•™æŒ‡å®šé—œéµå­—çš„å…¬å‘Š
    """
    url = build_list_url(page)
    print(f"\næŠ“åˆ—è¡¨é ï¼š{url}")

    html = safe_get(url)
    if html is None:
        print("åˆ—è¡¨é æŠ“å–å¤±æ•—ï¼Œè¦–ç‚ºç„¡è³‡æ–™")
        return []

    soup = BeautifulSoup(html, "html.parser")
    rows: List[Dict] = []

    for a in soup.find_all("a", href=True):
        title = a.get_text(strip=True)
        if not title:
            continue

      
        if not any(kw in title for kw in KEYWORDS):
            continue

        m = re.search(r"\d{3}\.\d{2}\.\d{2}", title)
        if not m:
            continue
        roc_date = m.group(0)

        article_url = requests.compat.urljoin(BASE_URL, a["href"])
        rows.append({"roc_date": roc_date, "url": article_url, "title": title})

    print(f"æœ¬é æŠ“åˆ° {len(rows)} ç­†")
    return rows



def clean_content(text: str) -> str:
    """æŠŠå…¬å‘Šå…§å®¹è£¡çš„æ›è¡Œã€å…¨å½¢ç©ºç™½ç­‰æ•´ç†æˆå–®è¡Œå­—ä¸²ã€‚"""
    if not isinstance(text, str):
        return ""

    
    text = text.replace("\r\n", "\n").replace("\r", "\n")

   
    text = text.replace("\n\u3000", "")

    
    text = text.replace("\n", " ")

    
    text = re.sub(r"\s+", " ", text)

    
    text = text.replace("\u3000", "")
    
    return text.strip()

def extract_maincontent_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    main = soup.select_one("div.maincontent")
    if not main:
        return ""

    text = " ".join(main.stripped_strings)

    #çµ±ä¸€æ¸…æ‰ä¸å¿…è¦çš„æ›è¡Œï¼ç©ºç™½
    text = clean_content(text)

    return text

def crawl_article(url: str) -> str:
    print(f"æŠ“æ–‡ç« é ï¼š{url}")
    html = safe_get(url)
    if html is None:
        return ""
    return extract_maincontent_text(html)



# åˆä½µmanual_gap.csvï¼ˆä»¥è¥¿å…ƒæ—¥æœŸç‚º key è¦†è“‹ï¼‰
# ------------------------------------------------------------
def load_manual_gap() -> pd.DataFrame | None:
    
    if not MANUAL_GAP.exists():
        print("â„¹ï¸ æ‰¾ä¸åˆ° manual_gap.csvï¼Œç•¥éè£œä¸è®€å–")
        return None

    print(f"è®€å–è£œä¸ï¼š{MANUAL_GAP}")
    gap = pd.read_csv(MANUAL_GAP, encoding="utf-8-sig")

    if "æ—¥æœŸ" not in gap.columns or "å…§å®¹" not in gap.columns:
        raise KeyError("manual_gap.csv è‡³å°‘è¦æœ‰æ—¥æœŸã€å…§å®¹å…©å€‹æ¬„ä½ã€‚")

    # çµ±ä¸€æ—¥æœŸæ ¼å¼
    gap = normalize_date_column(gap, "æ—¥æœŸ")

  
    if "æ¨™é¡Œ" not in gap.columns:
        gap["æ¨™é¡Œ"] = "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹"
    if "ä¾†æºç¶²å€" not in gap.columns:
        gap["ä¾†æºç¶²å€"] = ""

    #æ¬„ä½é †åºæ•´ç†
    gap = gap[["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]]

    print(f"è£œä¸ç­†æ•¸ï¼š{len(gap)}")
    return gap


def apply_manual_gap(base_df: pd.DataFrame) -> pd.DataFrame:
    
    gap_df = load_manual_gap()
    if gap_df is None or gap_df.empty:
        print("æ²’æœ‰è£œä¸æˆ–è£œä¸ç‚ºç©ºï¼Œç•¥éè£œä¸åˆä½µ")
       
        base_df = base_df.copy()
        for col in ["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]:
            if col not in base_df.columns:
                base_df[col] = ""
        base_df = base_df[["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]]
        #æ’åº
        base_df = base_df.sort_values("æ—¥æœŸ").reset_index(drop=True)
        return base_df

    base_df = base_df.copy()

    
    for col in ["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]:
        if col not in base_df.columns:
            base_df[col] = ""

    base_df = base_df[["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]]
    gap_df = gap_df[["æ—¥æœŸ", "æ¨™é¡Œ", "å…§å®¹", "ä¾†æºç¶²å€"]]

 
    gap_dates = gap_df["æ—¥æœŸ"].unique().tolist()
    before_len = len(base_df)
    base_df = base_df[~base_df["æ—¥æœŸ"].isin(gap_dates)].reset_index(drop=True)
    after_len = len(base_df)
    print(f"å¥—ç”¨è£œä¸ï¼šåˆªé™¤åŸæœ¬åŒæ—¥æœŸè³‡æ–™ {before_len - after_len} ç­†")

    merged_df = pd.concat([base_df, gap_df], ignore_index=True)

    #ä¾æ—¥æœŸæ’åº
    merged_df = merged_df.sort_values("æ—¥æœŸ").reset_index(drop=True)
    print(f"å¥—ç”¨è£œä¸å¾Œç¸½ç­†æ•¸ï¼š{len(merged_df)}")
    return merged_df


# ------------------------------------------------------------
# fullï¼šæŠ“åˆ°ã€Œæ²’æœ‰æ–°æ–‡ç« ã€å°±è‡ªå‹•åœ
# ------------------------------------------------------------
def run_full():
    print("[FULL] å…¨é‡æ¨¡å¼é–‹å§‹")

    all_rows: List[Dict] = []
    seen_urls: set[str] = set()
    page = 1
    consecutive_no_new = 0  

    while True:
        entries = crawl_list_page(page)
        if not entries:
            print("æ­¤é å®Œå…¨æ²’æœ‰ç¬¦åˆé—œéµå­—çš„æ–‡ç«  è¦–ç‚ºå°¾ç«¯ï¼Œåœæ­¢ã€‚")
            break

     
        new_entries = [e for e in entries if e["url"] not in seen_urls]

        if not new_entries:
            consecutive_no_new += 1
            print(f"ç¬¬ {page} é æ²’æœ‰æ–°æ–‡ç« ï¼ˆé€£çºŒ {consecutive_no_new} é ï¼‰ã€‚")

            #åœ‹é˜²éƒ¨åœ¨è¶…éæœ€å¾Œä¸€é æ™‚æœƒé‡è¤‡å›å‚³åŒä¸€é 

            if consecutive_no_new >= 2:
                print("ğŸ”š é€£çºŒå…©é éƒ½æ²’æœ‰æ–°ç¶²å€ï¼Œåˆ¤å®šå·²åˆ°æœ€å¾Œä¸€é ï¼Œåœæ­¢å¾€å¾ŒæŠ“ã€‚")
                break
        else:
            consecutive_no_new = 0

        for e in new_entries:
            seen_urls.add(e["url"])
            content = crawl_article(e["url"])

          
            roc_date_slash = e["roc_date"].replace(".", "/")
            iso_date = normalize_date_to_iso(roc_date_slash)

            all_rows.append(
                {
                    "æ—¥æœŸ": iso_date,
                    "æ¨™é¡Œ": e["title"],
                    "å…§å®¹": content,
                    "ä¾†æºç¶²å€": e["url"],
                }
            )
            time.sleep(0.3)

        print(f"ç´¯ç©ç­†æ•¸ï¼š{len(all_rows)}")
        page += 1
        time.sleep(0.5)

   
        if page > 1000:
            print("é æ•¸è¶…é 1000ï¼Œå¼·åˆ¶åœæ­¢ï¼ˆæ‡‰è©²ä¸æœƒç™¼ç”Ÿï¼‰ã€‚")
            break

    df = pd.DataFrame(all_rows)

    if df.empty:
        print("å…¨é‡çˆ¬å®Œçµæœç‚ºç©ºï¼Œè«‹æª¢æŸ¥åˆ—è¡¨ selector æˆ–ç¶²ç«™çµæ§‹ã€‚")
        # å»ºä¸€å€‹ç©ºçš„æ¨™æº–æ¬„ä½ CSVï¼Œè‡³å°‘ä¸æœƒç›´æ¥ç‚¸æ‰
        df = pd.DataFrame(columns=["æ—¥æœŸ", "å…¬å‘Šå…§å®¹"])
        df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
        print(f"è¼¸å‡ºç©º CSVï¼š{OUTPUT_CSV}")
        return

   
    df = apply_manual_gap(df)

    
    output_df = df[["æ—¥æœŸ", "å…§å®¹"]].rename(columns={"å…§å®¹": "å…¬å‘Šå…§å®¹"})

    # æ—¥æœŸç”±è¿‘åˆ°é æ’åº
    output_df = output_df.sort_values("æ—¥æœŸ", ascending=False).reset_index(drop=True)

    output_df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
    print(f"å…¨é‡å®Œæˆï¼Œè¼¸å‡º {OUTPUT_CSV}ï¼Œå…± {len(output_df)} ç­†")

# ------------------------------------------------------------
# dailyï¼šæ¯å¤©åªæŠ“æœ€æ–°ä¸€ç­†
# ------------------------------------------------------------
def run_daily():
    print("[DAILY] æ¯æ—¥æ¨¡å¼é–‹å§‹ï¼ˆåªæŠ“æœ€æ–°ä¸€ç­†ï¼‰")

    entries = crawl_list_page(1)
    if not entries:
        print("ç¬¬ 1 é æŠ“ä¸åˆ°è³‡æ–™ï¼Œä»Šæ—¥ç•¥éã€‚")
        return

    newest = entries[0]

    roc_date_slash = newest["roc_date"].replace(".", "/")
    iso_date = normalize_date_to_iso(roc_date_slash)
    content = crawl_article(newest["url"])

    df_new = pd.DataFrame(
        [
            {
                "æ—¥æœŸ": iso_date,
                "æ¨™é¡Œ": newest["title"],
                "å…§å®¹": content,
                "ä¾†æºç¶²å€": newest["url"],
            }
        ]
    )

    if OUTPUT_CSV.exists():
        df_old = pd.read_csv(OUTPUT_CSV, encoding="utf-8-sig")

  
        if "å…§å®¹" not in df_old.columns and "å…¬å‘Šå…§å®¹" in df_old.columns:
            df_old["å…§å®¹"] = df_old["å…¬å‘Šå…§å®¹"]

        if "æ¨™é¡Œ" not in df_old.columns:
            df_old["æ¨™é¡Œ"] = ""
        if "ä¾†æºç¶²å€" not in df_old.columns:
            df_old["ä¾†æºç¶²å€"] = ""
        if df_old["æ—¥æœŸ"].astype(str).str.contains(r"/").any():
         
            try:
                df_old = normalize_date_column(df_old, "æ—¥æœŸ")
            except Exception as e:
                print(f"èˆŠæª”æ—¥æœŸè½‰æ›å¤±æ•—ï¼š{e}")

        df = pd.concat([df_old, df_new], ignore_index=True)

    else:
        df = df_new


    df = apply_manual_gap(df)

  
    output_df = df[["æ—¥æœŸ", "å…§å®¹"]].rename(columns={"å…§å®¹": "å…¬å‘Šå…§å®¹"})


    output_df = output_df.sort_values("æ—¥æœŸ", ascending=False).reset_index(drop=True)

    output_df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

    print(f"æ¯æ—¥æ›´æ–°å®Œæˆï¼Œç¾åœ¨ {OUTPUT_CSV} å…± {len(output_df)} ç­†")

# ------------------------------------------------------------
# main
# ------------------------------------------------------------
if __name__ == "__main__":
    # python mnd_crawler.py full  å…¨é‡
    # python mnd_crawler.py       æ¯æ—¥æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "full":
        run_full()
    else:
        run_daily()
