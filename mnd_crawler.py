#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
mnd_crawler.py

åŠŸèƒ½ï¼š
- fullï¼šæŠŠç›®å‰ç¶²ç«™ä¸Šæ‰€æœ‰ã€Œå€åŸŸå‹•æ…‹ã€çš„å…±æ©Ÿ/æµ·åŸŸå…¬å‘ŠæŠ“ä¸‹ä¾† â†’ mnd_pla.csv
- dailyï¼šæ¯å¤©åªæŠ“æœ€æ–°ä¸€ç­†ï¼Œappend åˆ° mnd_pla.csv
- manual_gap.csvï¼šè£œä¸äº†çš„æ—¥æœŸç”¨é€™å€‹è£œï¼Œæœ€å¾Œæœƒä¸€èµ· merge é€² mnd_pla.csv

ç‰¹åˆ¥è™•ç†ï¼š
- åˆ—è¡¨é ç”¨ https://www.mnd.gov.tw/news/plaactlist (+ /2, /3, ...)
- å…§é åªæŠ“ <div class="maincontent">
- è£œä¸æª” manual_gap.csv è¦æœ‰å…©æ¬„ï¼šæ—¥æœŸ, å…§å®¹ï¼ˆå«æ¨™é¡Œåˆ—ï¼‰
"""

import sys
import time
import re
from pathlib import Path
from typing import List, Dict

import requests
from bs4 import BeautifulSoup
import pandas as pd

# ------------------------------------------------------------
# å¸¸æ•¸è¨­å®š
# ------------------------------------------------------------
BASE_URL = "https://www.mnd.gov.tw"
LIST_BASE = f"{BASE_URL}/news/plaactlist"
HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"}

BASE_DIR = Path(__file__).parent
OUTPUT_CSV = BASE_DIR / "mnd_pla.csv"
MANUAL_GAP = BASE_DIR / "manual_gap.csv"

# ä½ åŸæœ¬ ASPX ç‰ˆæœ¬çš„é—œéµå­—ï¼ˆç¢ºå®šæŠ“å¾—åˆ°æ‰€æœ‰ç‰ˆæœ¬çš„å…±æ©Ÿå…¬å‘Šï¼‰
KEYWORDS = [
    "ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹",
    "ä¸­å…±è§£æ”¾è»è»æ©Ÿ",
    "ä¸­å…±è§£æ”¾è»é€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "è¸°è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "é€¾è¶Šæµ·å³½ä¸­ç·šåŠé€²å…¥æˆ‘è¥¿å—ç©ºåŸŸæ´»å‹•æƒ…æ³",
    "æˆ‘è¥¿å—ç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "è‡ºæµ·å‘¨é‚Šç©ºåŸŸç©ºæƒ…å‹•æ…‹",
    "åµç²å…±æ©Ÿã€è‰¦åœ¨è‡ºæµ·å‘¨é‚Šæ´»å‹•æƒ…å½¢",
]


# ------------------------------------------------------------
# GET with retry
# ------------------------------------------------------------
def safe_get(url: str, retries: int = 3, timeout: int = 20) -> str | None:
    for i in range(1, retries + 1):
        try:
            r = requests.get(url, headers=HEADERS, timeout=timeout)
            r.raise_for_status()
            r.encoding = r.apparent_encoding
            return r.text
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i} æ¬¡å¤±æ•—ï¼š{url} - {e}")
            time.sleep(1)
    print(f"âŒ æœ€çµ‚å¤±æ•—ï¼š{url}")
    return None


# ------------------------------------------------------------
# åˆ—è¡¨é 
# ------------------------------------------------------------
def build_list_url(page: int) -> str:
    # page=1: /plaactlist, page>=2: /plaactlist/2
    return LIST_BASE if page == 1 else f"{LIST_BASE}/{page}"


def crawl_list_page(page: int) -> List[Dict]:
    """
    æŠ“æŸä¸€é åˆ—è¡¨ï¼Œåªç•™ä½ æŒ‡å®šé—œéµå­—çš„å…¬å‘Š
    å›å‚³ list[dict]: {"roc_date": "114.12.01", "url": "..."}
    """
    url = build_list_url(page)
    print(f"\nğŸ” æŠ“åˆ—è¡¨é ï¼š{url}")

    html = safe_get(url)
    if html is None:
        print("âš ï¸ åˆ—è¡¨é æŠ“å–å¤±æ•—ï¼Œè¦–ç‚ºç„¡è³‡æ–™")
        return []

    soup = BeautifulSoup(html, "html.parser")
    rows: List[Dict] = []

    for a in soup.find_all("a", href=True):
        title = a.get_text(strip=True)
        if not any(kw in title for kw in KEYWORDS):
            continue

        # ä¾‹å¦‚ï¼š114.12.01ä¸­å…±è§£æ”¾è»è‡ºæµ·å‘¨é‚Šæµ·ã€ç©ºåŸŸå‹•æ…‹é»é–±æ¬¡æ•¸ï¼š413 æ¬¡
        m = re.search(r"\d{3}\.\d{2}\.\d{2}", title)
        if not m:
            continue
        roc_date = m.group(0)

        article_url = requests.compat.urljoin(BASE_URL, a["href"])
        rows.append({"roc_date": roc_date, "url": article_url})

    print(f"ğŸ“Œ æœ¬é æŠ“åˆ° {len(rows)} ç­†")
    return rows


# ------------------------------------------------------------
# å…§é ï¼šåªæŠ“ maincontent
# ------------------------------------------------------------
def extract_maincontent_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    main = soup.select_one("div.maincontent")
    if not main:
        return ""

    text = " ".join(main.stripped_strings)
    return text


def crawl_article(url: str) -> str:
    print(f"â¡ï¸ æŠ“æ–‡ç« é ï¼š{url}")
    html = safe_get(url)
    if html is None:
        return ""
    return extract_maincontent_text(html)


# ------------------------------------------------------------
# æ°‘åœ‹æ—¥æœŸæ’åº key
# ------------------------------------------------------------
def roc_sort_key(s: str):
    try:
        y, m, d = s.split("/")
        return int(y), int(m), int(d)
    except Exception:
        return (0, 0, 0)


# ------------------------------------------------------------
# åˆä½µè£œä¸ manual_gap.csv
# ------------------------------------------------------------
def apply_manual_gap(df: pd.DataFrame) -> pd.DataFrame:
    """
    manual_gap.csv çµæ§‹ï¼š
    æ—¥æœŸ,å…§å®¹
    109/01/01,....
    """
    if MANUAL_GAP.exists():
        print(f"ğŸ“¥ åˆä½µè£œä¸ï¼š{MANUAL_GAP}")
        gap = pd.read_csv(MANUAL_GAP, encoding="utf-8-sig")
        df = pd.concat([df, gap], ignore_index=True)
    else:
        print("â„¹ï¸ æ‰¾ä¸åˆ° manual_gap.csvï¼Œç•¥éè£œä¸åˆä½µ")

    df = df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
    df = df.sort_values("æ—¥æœŸ", key=lambda col: col.map(roc_sort_key))
    return df.reset_index(drop=True)


# ------------------------------------------------------------
# fullï¼šæŠ“åˆ°ã€Œæ²’æœ‰æ–°æ–‡ç« ã€å°±è‡ªå‹•åœ
# ------------------------------------------------------------
def run_full():
    print("ğŸš€ [FULL] å…¨é‡æ¨¡å¼é–‹å§‹")

    all_rows: List[Dict] = []
    seen_urls: set[str] = set()
    page = 1
    consecutive_no_new = 0  # é€£çºŒå¹¾é ã€Œæ²’æœ‰æ–°æ–‡ç« ã€

    while True:
        entries = crawl_list_page(page)
        if not entries:
            print("âšª æ­¤é å®Œå…¨æ²’æœ‰ç¬¦åˆé—œéµå­—çš„æ–‡ç«  â†’ è¦–ç‚ºå°¾ç«¯ï¼Œåœæ­¢ã€‚")
            break

        # åªç•™ä¸‹æ²’çœ‹éçš„ url
        new_entries = [e for e in entries if e["url"] not in seen_urls]

        if not new_entries:
            consecutive_no_new += 1
            print(f"âšª ç¬¬ {page} é æ²’æœ‰æ–°æ–‡ç« ï¼ˆé€£çºŒ {consecutive_no_new} é ï¼‰ã€‚")

            # åœ‹é˜²éƒ¨åœ¨è¶…éæœ€å¾Œä¸€é æ™‚æœƒé‡è¤‡å›å‚³åŒä¸€é 
            # â†’ é€£çºŒå…©é éƒ½æ²’æœ‰æ–°ç¶²å€ï¼Œå°±è¦–ç‚ºå·²ç¶“åˆ·åˆ°æœ€å¾Œä¸€é 
            if consecutive_no_new >= 2:
                print("ğŸ”š é€£çºŒå…©é éƒ½æ²’æœ‰æ–°ç¶²å€ï¼Œåˆ¤å®šå·²åˆ°æœ€å¾Œä¸€é ï¼Œåœæ­¢å¾€å¾ŒæŠ“ã€‚")
                break
        else:
            consecutive_no_new = 0

        for e in new_entries:
            seen_urls.add(e["url"])
            content = crawl_article(e["url"])
            date_str = e["roc_date"].replace(".", "/")
            all_rows.append({"æ—¥æœŸ": date_str, "å…§å®¹": content})
            time.sleep(0.3)

        print(f"ğŸ“Š ç´¯ç©ç­†æ•¸ï¼š{len(all_rows)}")
        page += 1
        time.sleep(0.5)

        # å®‰å…¨ä¿éšªï¼šé˜²ç¦¦æ€§ä¸Šé™ï¼Œé¿å…æ„å¤– infinite loop
        if page > 1000:
            print("âš ï¸ é æ•¸è¶…é 1000ï¼Œå¼·åˆ¶åœæ­¢ï¼ˆæ‡‰è©²ä¸æœƒç™¼ç”Ÿï¼‰ã€‚")
            break

    df = pd.DataFrame(all_rows)
    df = apply_manual_gap(df)
    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

    print(f"âœ… å…¨é‡å®Œæˆï¼Œè¼¸å‡º {OUTPUT_CSV}ï¼Œå…± {len(df)} ç­†")


# ------------------------------------------------------------
# dailyï¼šæ¯å¤©åªæŠ“æœ€æ–°ä¸€ç­†
# ------------------------------------------------------------
def run_daily():
    print("ğŸ“… [DAILY] æ¯æ—¥æ¨¡å¼é–‹å§‹ï¼ˆåªæŠ“æœ€æ–°ä¸€ç­†ï¼‰")

    entries = crawl_list_page(1)
    if not entries:
        print("âš ï¸ ç¬¬ 1 é æŠ“ä¸åˆ°è³‡æ–™ï¼Œä»Šæ—¥ç•¥éã€‚")
        return

    newest = entries[0]
    date_str = newest["roc_date"].replace(".", "/")
    content = crawl_article(newest["url"])

    df_new = pd.DataFrame([{"æ—¥æœŸ": date_str, "å…§å®¹": content}])

    if OUTPUT_CSV.exists():
        df_old = pd.read_csv(OUTPUT_CSV, encoding="utf-8-sig")
        df = pd.concat([df_old, df_new], ignore_index=True)
    else:
        df = df_new

    df = apply_manual_gap(df)
    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

    print(f"âœ… æ¯æ—¥æ›´æ–°å®Œæˆï¼Œç¾åœ¨ {OUTPUT_CSV} å…± {len(df)} ç­†")


# ------------------------------------------------------------
# main
# ------------------------------------------------------------
if __name__ == "__main__":
    # python mnd_crawler.py full  â†’ å…¨é‡
    # python mnd_crawler.py       â†’ æ¯æ—¥æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "full":
        run_full()
    else:
        run_daily()
